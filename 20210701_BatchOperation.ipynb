{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hazardous-stanley",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import statistics\n",
    "%matplotlib inline\n",
    "from datetime import datetime\n",
    "from pandas import Series\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "import seaborn as sns\n",
    "import os\n",
    "import statistics\n",
    "\n",
    "from sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error, mean_squared_error, mean_squared_log_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from dateutil.relativedelta import relativedelta \n",
    "from scipy.optimize import minimize              \n",
    "\n",
    "import statsmodels.formula.api as smf          \n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as scs\n",
    "\n",
    "from itertools import product      \n",
    "\n",
    "from statsmodels.tsa.api import ExponentialSmoothing,SimpleExpSmoothing, Holt\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from matplotlib.pylab import rcParams\n",
    "\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "random-merchant",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    squared_error = 0\n",
    "    for i in range(len(y_true)):\n",
    "        squared_error = squared_error + (y_true[i] - y_pred[i]) ** 2\n",
    "    root_mean_squared_error = sqrt(squared_error / len(y_true))\n",
    "    return root_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "freelance-mounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_avg(rounds):\n",
    "    y_hat['avg'] = 0\n",
    "    for i in range(len(Valid)):\n",
    "        y_pred = 0\n",
    "        for j in range(rounds):\n",
    "            y_pred = y_pred + train.footfall[len(Train) - (j + 1) * len(Valid) + i]\n",
    "        y_hat['avg'][i] = y_pred / rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "graphic-mountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_stationary(timeseries):\n",
    "    #Determine rolling statistics\n",
    "    rolmean = pd.Series(timeseries).rolling(window = 28).mean()\n",
    "    rolstd = pd.Series(timeseries).rolling(window = 28).std()\n",
    "    \n",
    "    #Plot rolling Statistics\n",
    "    orig = plt.plot(timeseries, color = \"blue\", label = \"Original\")\n",
    "    mean = plt.plot(rolmean, color = \"red\", label = \"Rolling Mean\")\n",
    "    std = plt.plot(rolstd, color = \"black\", label = \"Rolling Std\")\n",
    "    plt.legend(loc = \"best\")\n",
    "    plt.title(\"Rolling Mean and Standard Deviation\")\n",
    "    plt.show(block = False)\n",
    "    \n",
    "    #Perform Dickey Fuller test\n",
    "    print(\"Results of Dickey Fuller test: \")\n",
    "    dftest = adfuller(timeseries, autolag = 'AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index = ['Test Statistics', 'p-value', '# Lag Used', 'Number of Observations Used'])\n",
    "    \n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)' %key] = value\n",
    "    print(dfoutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-financing",
   "metadata": {},
   "source": [
    "### Exclude Locations with Too Much Missing Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "authentic-lightweight",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = pd.read_csv('stackfootfall.csv')\n",
    "ff.timestamp = pd.to_datetime(ff.timestamp, format = '%Y-%m-%d %H:%M:%S') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "white-passion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>location</th>\n",
       "      <th>device</th>\n",
       "      <th>footfall</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>day_of_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-05 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-05 01:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-01-05 02:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2018-01-05 03:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2018-01-05 04:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8896171</th>\n",
       "      <td>260179</td>\n",
       "      <td>2019-08-18 19:00:00</td>\n",
       "      <td>1228.0</td>\n",
       "      <td>1074.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>2019</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8896172</th>\n",
       "      <td>260180</td>\n",
       "      <td>2019-08-18 20:00:00</td>\n",
       "      <td>1228.0</td>\n",
       "      <td>1074.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>2019</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8896173</th>\n",
       "      <td>260181</td>\n",
       "      <td>2019-08-18 21:00:00</td>\n",
       "      <td>1228.0</td>\n",
       "      <td>1074.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>2019</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8896174</th>\n",
       "      <td>260182</td>\n",
       "      <td>2019-08-18 22:00:00</td>\n",
       "      <td>1228.0</td>\n",
       "      <td>1074.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>2019</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8896175</th>\n",
       "      <td>260183</td>\n",
       "      <td>2019-08-18 23:00:00</td>\n",
       "      <td>1228.0</td>\n",
       "      <td>1074.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2019</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8800152 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0           timestamp  location  device  footfall  year  \\\n",
       "0                 0 2018-01-05 00:00:00       0.0  1240.0       NaN  2018   \n",
       "1                 1 2018-01-05 01:00:00       0.0  1240.0       NaN  2018   \n",
       "2                 2 2018-01-05 02:00:00       0.0  1240.0       NaN  2018   \n",
       "3                 3 2018-01-05 03:00:00       0.0  1240.0       NaN  2018   \n",
       "4                 4 2018-01-05 04:00:00       0.0  1240.0       NaN  2018   \n",
       "...             ...                 ...       ...     ...       ...   ...   \n",
       "8896171      260179 2019-08-18 19:00:00    1228.0  1074.0     310.0  2019   \n",
       "8896172      260180 2019-08-18 20:00:00    1228.0  1074.0     137.0  2019   \n",
       "8896173      260181 2019-08-18 21:00:00    1228.0  1074.0     120.0  2019   \n",
       "8896174      260182 2019-08-18 22:00:00    1228.0  1074.0      46.0  2019   \n",
       "8896175      260183 2019-08-18 23:00:00    1228.0  1074.0      12.0  2019   \n",
       "\n",
       "         month  day  hour  day_of_week  \n",
       "0            1    5     0            5  \n",
       "1            1    5     1            5  \n",
       "2            1    5     2            5  \n",
       "3            1    5     3            5  \n",
       "4            1    5     4            5  \n",
       "...        ...  ...   ...          ...  \n",
       "8896171      8   18    19            7  \n",
       "8896172      8   18    20            7  \n",
       "8896173      8   18    21            7  \n",
       "8896174      8   18    22            7  \n",
       "8896175      8   18    23            7  \n",
       "\n",
       "[8800152 rows x 10 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff = ff.loc[(ff.timestamp >= '2018-01') & (ff.timestamp <= '2019-08-18 23:00')]\n",
    "\n",
    "ff_count = ff.groupby('location')['footfall'].count() / max(ff.groupby('location')['footfall'].count())\n",
    "limit1 = ff_count.loc[ff_count >= 0.6].to_frame()\n",
    "ff_test = ff.loc[(ff.timestamp >= '2018-07-29') & (ff.timestamp <= '2019-08-18 23:00')]\n",
    "ff_test_count = ff_test.groupby('location')['footfall'].count() / max(ff_test.groupby('location')['footfall'].count())\n",
    "limit2 = ff_test_count.loc[ff_test_count >= 0.8].to_frame()\n",
    "limit = pd.merge(limit1, limit2, how = 'inner', on = 'location')\n",
    "\n",
    "ff = pd.merge(ff, limit, how = 'inner', on = 'location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "appropriate-powder",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(columns = ('location','Naive','Moving_Average12','Moving_Average24',\n",
    "                                 'Moving_Average30','Expoential_Smoothing','SARIMA'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-pantyhose",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ff.location:\n",
    "     \n",
    "    ff_i = ff.loc[(ff.location <= i) & (ff.location >= i)][[\n",
    "        'timestamp','footfall','year','month','day','hour','day_of_week']]    \n",
    "    ff_i.index = ff_i.timestamp\n",
    "    ff_i = ff_i.resample('H').mean()\n",
    "    \n",
    "    ff_i['footfall'].replace(0, np.nan, inplace = True)\n",
    "    ff_i['footfall'] = ff_i['footfall'].interpolate(method='linear')\n",
    "    ff_i = ff_i.reset_index(level = ['timestamp'])\n",
    "    ff_i.timestamp = pd.to_datetime(ff_i.timestamp, format = '%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    temp = ff_i[['timestamp', 'footfall']]\n",
    "    temp.index = temp.timestamp\n",
    "    temp['year'] = temp.timestamp.dt.year\n",
    "    temp['month'] = temp.timestamp.dt.month\n",
    "    temp['day'] = temp.timestamp.dt.day\n",
    "    temp['hour'] = temp.timestamp.dt.hour\n",
    "    temp['day_of_week'] = temp.timestamp.dt.dayofweek + 1\n",
    "    \n",
    "    Train = temp.loc[(temp.timestamp >= '2018-01-01 00:00') & (temp.timestamp <= '2019-07-28 23:00')]\n",
    "    Valid = temp.loc[(temp.timestamp >= '2019-07-29 00:00') & (temp.timestamp <= '2019-08-18 23:00')]\n",
    "    \n",
    "    train = Train.resample('D').mean()\n",
    "    valid = Valid.resample('D').mean() \n",
    "    \n",
    "    train['footfall'] = train['footfall'].interpolate(method='linear')\n",
    "    valid['footfall'] = valid['footfall'].interpolate(method='linear')\n",
    "\n",
    "    # Monday to Thursday Ratio\n",
    "    mtt = Train.loc[(Train.day_of_week >= 1) & (Train.day_of_week <= 4)].groupby('hour')['footfall'].mean().to_frame()\n",
    "    mtt['ratio'] = mtt['footfall']/mtt['footfall'].sum()\n",
    "    mtt = mtt.reset_index(level = ['hour'])\n",
    "\n",
    "    # Friday Ratio\n",
    "    fri = Train.loc[(Train.day_of_week >= 5) & (Train.day_of_week <= 5)].groupby('hour')['footfall'].mean().to_frame()\n",
    "    fri['ratio'] = fri['footfall']/fri['footfall'].sum()\n",
    "    fri = fri.reset_index(level = ['hour'])\n",
    "\n",
    "    # Sat Ratio\n",
    "    sat = Train.loc[(Train.day_of_week >= 6) & (Train.day_of_week <= 6)].groupby('hour')['footfall'].mean().to_frame()\n",
    "    sat['ratio'] = sat['footfall']/sat['footfall'].sum()\n",
    "    sat = sat.reset_index(level = ['hour'])\n",
    "\n",
    "    # Sun Ratio\n",
    "    sun = Train.loc[(Train.day_of_week >= 7) & (Train.day_of_week <= 7)].groupby('hour')['footfall'].mean().to_frame()\n",
    "    sun['ratio'] = sun['footfall']/sun['footfall'].sum()\n",
    "    sun = sun.reset_index(level = ['hour'])\n",
    "\n",
    "    ratio1 = mtt[['hour','ratio']]\n",
    "    ratio1['day_of_week'] = 1\n",
    "    ratio2 = mtt[['hour','ratio']]\n",
    "    ratio2['day_of_week'] = 2\n",
    "    ratio3 = mtt[['hour','ratio']]\n",
    "    ratio3['day_of_week'] = 3\n",
    "    ratio4 = mtt[['hour','ratio']]\n",
    "    ratio4['day_of_week'] = 4\n",
    "    ratio5 = fri[['hour','ratio']]\n",
    "    ratio5['day_of_week'] = 5\n",
    "    ratio6 = sat[['hour','ratio']]\n",
    "    ratio6['day_of_week'] = 6\n",
    "    ratio7 = sun[['hour','ratio']]\n",
    "    ratio7['day_of_week'] = 7\n",
    "\n",
    "    ratio_week = ratio1.append(ratio2).append(ratio3).append(ratio4).append(ratio5).append(ratio6).append(ratio7)\n",
    "\n",
    "    merge = pd.merge(Valid, ratio_week, on = ('hour','day_of_week'), how = 'left')\n",
    "\n",
    "    # Naive Approach\n",
    "    dd = np.asarray(Train['footfall'])\n",
    "    y_hat = Valid.copy()\n",
    "    y_hat['naive']= dd[len(dd)- 1]\n",
    "    for i in range(len(Valid)): \n",
    "        y_hat['naive'][i]= dd[len(dd) - len(Valid) + i]\n",
    "    naive_rmse = rmse(Valid.footfall, y_hat.naive)\n",
    "    \n",
    "    # Moving Average Approach\n",
    "    moving_avg(4)\n",
    "    avg4_rmse = rmse(Valid.footfall, y_hat.avg)\n",
    "    moving_avg(8)\n",
    "    avg8_rmse = rmse(Valid.footfall, y_hat.avg)\n",
    "    moving_avg(10)\n",
    "    avg10_rmse = rmse(Valid.footfall, y_hat.avg)\n",
    "    \n",
    "    # Exponential Smoothing\n",
    "    y_hat = valid.copy()\n",
    "    fit1 = ExponentialSmoothing(np.asarray(train.footfall), seasonal_periods = 7, trend = 'add', seasonal= 'add').fit()\n",
    "    y_hat['Holt_Winter'] = fit1.forecast(len(valid))    \n",
    "    HoltWinter_rmse = rmse(valid.footfall, y_hat.Holt_Winter)\n",
    "    \n",
    "    pred_d = y_hat.Holt_Winter.to_frame()\n",
    "    pred_d = pred_d.reset_index(level = ['timestamp'])\n",
    "    pred_d['month'] = pred_d.timestamp.dt.month\n",
    "    pred_d['day'] = pred_d.timestamp.dt.day\n",
    "    temp = pd.merge(merge, pred_d, on = ('month','day'), how = 'left')\n",
    "    temp['prediction'] = temp['ratio'] * temp['Holt_Winter'] * 24\n",
    "    holt_winter_rmse = rmse(temp['footfall'], temp['prediction'])\n",
    "    \n",
    "    # SARIMA\n",
    "    fit1 = sm.tsa.statespace.SARIMAX(train.footfall, order = (1,1,3), seasonal_order =(1,1,3,7)).fit()\n",
    "    y_hat['SARIMA'] = fit1.predict(start = \"2019-07-29 00:00\", end = \"2019-08-18 23:00\", dynamic=True)\n",
    "    SARIMA_rmse = rmse(valid['footfall'], y_hat['SARIMA'])\n",
    "    \n",
    "    pred_d = y_hat['SARIMA'].to_frame()\n",
    "    pred_d = pred_d.reset_index(level = ['timestamp'])\n",
    "    pred_d['month'] = pred_d.timestamp.dt.month\n",
    "    pred_d['day'] = pred_d.timestamp.dt.day\n",
    "    temp2 = pd.merge(merge, pred_d, on = ('month','day'), how = 'left')\n",
    "    temp2['prediction'] = temp2['ratio'] * temp2['SARIMA'] * 24\n",
    "    sarima_rmse = rmse(temp2['footfall'], temp2['prediction'])\n",
    "    \n",
    "#    result.location[j] = i\n",
    "#    result.Naive[j] = naive_rmse\n",
    "#    result.Moving_Average12[j] = avg4_rmse\n",
    "#    result.Moving_Average24[j] = avg8_rmse\n",
    "#    result.Moving_Average30[j] = avg10_rmse\n",
    "#    result.Expoential_Smoothing[j] = holt_winter_rmse\n",
    "#    result.SARIMA[j] = sarima_rmse\n",
    "    result = result.append([i, naive_rmse, avg4_rmse, avg8_rmse, avg10_rmse, holt_winter_rmse, sarima_rmse],\n",
    "                           ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-finland",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-skirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('BatchOperation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failing-hawaiian",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urbsim",
   "language": "python",
   "name": "urbsim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
