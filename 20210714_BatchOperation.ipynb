{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hazardous-stanley",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import statistics\n",
    "from datetime import datetime\n",
    "from pandas import Series\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "import seaborn as sns\n",
    "import os\n",
    "import statistics\n",
    "\n",
    "from sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error, mean_squared_error, mean_squared_log_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from dateutil.relativedelta import relativedelta \n",
    "from scipy.optimize import minimize              \n",
    "\n",
    "import statsmodels.formula.api as smf          \n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as scs\n",
    "\n",
    "from itertools import product      \n",
    "\n",
    "from statsmodels.tsa.api import ExponentialSmoothing,SimpleExpSmoothing, Holt\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from matplotlib.pylab import rcParams\n",
    "\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from pmdarima.arima import auto_arima\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "jewish-mattress",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "register_matplotlib_converters()\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "\n",
    "rcParams['figure.figsize'] = 22, 10\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "laughing-restriction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(X, y, time_steps = 1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X.iloc[i:(i + time_steps)].values\n",
    "        Xs.append(v)        \n",
    "        ys.append(y.iloc[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "random-merchant",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    squared_error = 0\n",
    "    for i in range(len(y_true)):\n",
    "        squared_error = squared_error + (y_true[i] - y_pred[i]) ** 2\n",
    "    root_mean_squared_error = sqrt(squared_error / len(y_true))\n",
    "    return root_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "freelance-mounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_avg(rounds):\n",
    "    y_hat['avg'] = 0\n",
    "    for i in range(len(Valid)):\n",
    "        y_pred = 0\n",
    "        for j in range(rounds):\n",
    "            y_pred = y_pred + Train.footfall[len(Train) - (j + 1) * len(Valid) + i]\n",
    "        y_hat['avg'][i] = y_pred / rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "graphic-mountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_stationary(timeseries):\n",
    "    #Determine rolling statistics\n",
    "    rolmean = pd.Series(timeseries).rolling(window = 28).mean()\n",
    "    rolstd = pd.Series(timeseries).rolling(window = 28).std()\n",
    "    \n",
    "    #Plot rolling Statistics\n",
    "    orig = plt.plot(timeseries, color = \"blue\", label = \"Original\")\n",
    "    mean = plt.plot(rolmean, color = \"red\", label = \"Rolling Mean\")\n",
    "    std = plt.plot(rolstd, color = \"black\", label = \"Rolling Std\")\n",
    "    plt.legend(loc = \"best\")\n",
    "    plt.title(\"Rolling Mean and Standard Deviation\")\n",
    "    plt.show(block = False)\n",
    "    \n",
    "    #Perform Dickey Fuller test\n",
    "    print(\"Results of Dickey Fuller test: \")\n",
    "    dftest = adfuller(timeseries, autolag = 'AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index = ['Test Statistics', 'p-value', '# Lag Used', 'Number of Observations Used'])\n",
    "    \n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)' %key] = value\n",
    "    print(dfoutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "outer-response",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dm_test(actual_lst, pred1_lst, pred2_lst, h = 1, crit=\"MSE\", power = 2):\n",
    "    # Routine for checking errors\n",
    "    def error_check():\n",
    "        rt = 0\n",
    "        msg = \"\"\n",
    "        # Check if h is an integer\n",
    "        if (not isinstance(h, int)):\n",
    "            rt = -1\n",
    "            msg = \"The type of the number of steps ahead (h) is not an integer.\"\n",
    "            return (rt,msg)\n",
    "        # Check the range of h\n",
    "        if (h < 1):\n",
    "            rt = -1\n",
    "            msg = \"The number of steps ahead (h) is not large enough.\"\n",
    "            return (rt,msg)\n",
    "        len_act = len(actual_lst)\n",
    "        len_p1  = len(pred1_lst)\n",
    "        len_p2  = len(pred2_lst)\n",
    "        # Check if lengths of actual values and predicted values are equal\n",
    "        if (len_act != len_p1 or len_p1 != len_p2 or len_act != len_p2):\n",
    "            rt = -1\n",
    "            msg = \"Lengths of actual_lst, pred1_lst and pred2_lst do not match.\"\n",
    "            return (rt,msg)\n",
    "        # Check range of h\n",
    "        if (h >= len_act):\n",
    "            rt = -1\n",
    "            msg = \"The number of steps ahead is too large.\"\n",
    "            return (rt,msg)\n",
    "        # Check if criterion supported\n",
    "        if (crit != \"MSE\" and crit != \"MAPE\" and crit != \"MAD\" and crit != \"poly\"):\n",
    "            rt = -1\n",
    "            msg = \"The criterion is not supported.\"\n",
    "            return (rt,msg)  \n",
    "        # Check if every value of the input lists are numerical values\n",
    "        from re import compile as re_compile\n",
    "        comp = re_compile(\"^\\d+?\\.\\d+?$\")  \n",
    "        def compiled_regex(s):\n",
    "            \"\"\" Returns True is string is a number. \"\"\"\n",
    "            if comp.match(s) is None:\n",
    "                return s.isdigit()\n",
    "            return True\n",
    "        for actual, pred1, pred2 in zip(actual_lst, pred1_lst, pred2_lst):\n",
    "            is_actual_ok = compiled_regex(str(abs(actual)))\n",
    "            is_pred1_ok = compiled_regex(str(abs(pred1)))\n",
    "            is_pred2_ok = compiled_regex(str(abs(pred2)))\n",
    "            if (not (is_actual_ok and is_pred1_ok and is_pred2_ok)):  \n",
    "                msg = \"An element in the actual_lst, pred1_lst or pred2_lst is not numeric.\"\n",
    "                rt = -1\n",
    "                return (rt,msg)\n",
    "        return (rt,msg)\n",
    "    \n",
    "    # Error check\n",
    "    error_code = error_check()\n",
    "    # Raise error if cannot pass error check\n",
    "    if (error_code[0] == -1):\n",
    "        raise SyntaxError(error_code[1])\n",
    "        return\n",
    "    # Import libraries\n",
    "    from scipy.stats import t\n",
    "    import collections\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    # Initialise lists\n",
    "    e1_lst = []\n",
    "    e2_lst = []\n",
    "    d_lst  = []\n",
    "    \n",
    "    # convert every value of the lists into real values\n",
    "    actual_lst = pd.Series(actual_lst).apply(lambda x: float(x)).tolist()\n",
    "    pred1_lst = pd.Series(pred1_lst).apply(lambda x: float(x)).tolist()\n",
    "    pred2_lst = pd.Series(pred2_lst).apply(lambda x: float(x)).tolist()\n",
    "    \n",
    "    # Length of lists (as real numbers)\n",
    "    T = float(len(actual_lst))\n",
    "    \n",
    "    # construct d according to crit\n",
    "    if (crit == \"MSE\"):\n",
    "        for actual,p1,p2 in zip(actual_lst,pred1_lst,pred2_lst):\n",
    "            e1_lst.append((actual - p1)**2)\n",
    "            e2_lst.append((actual - p2)**2)\n",
    "        for e1, e2 in zip(e1_lst, e2_lst):\n",
    "            d_lst.append(e1 - e2)\n",
    "    elif (crit == \"MAD\"):\n",
    "        for actual,p1,p2 in zip(actual_lst,pred1_lst,pred2_lst):\n",
    "            e1_lst.append(abs(actual - p1))\n",
    "            e2_lst.append(abs(actual - p2))\n",
    "        for e1, e2 in zip(e1_lst, e2_lst):\n",
    "            d_lst.append(e1 - e2)\n",
    "    elif (crit == \"MAPE\"):\n",
    "        for actual,p1,p2 in zip(actual_lst,pred1_lst,pred2_lst):\n",
    "            e1_lst.append(abs((actual - p1)/actual))\n",
    "            e2_lst.append(abs((actual - p2)/actual))\n",
    "        for e1, e2 in zip(e1_lst, e2_lst):\n",
    "            d_lst.append(e1 - e2)\n",
    "    elif (crit == \"poly\"):\n",
    "        for actual,p1,p2 in zip(actual_lst,pred1_lst,pred2_lst):\n",
    "            e1_lst.append(((actual - p1))**(power))\n",
    "            e2_lst.append(((actual - p2))**(power))\n",
    "        for e1, e2 in zip(e1_lst, e2_lst):\n",
    "            d_lst.append(e1 - e2)    \n",
    "    \n",
    "    # Mean of d        \n",
    "    mean_d = pd.Series(d_lst).mean()\n",
    "    \n",
    "    # Find autocovariance and construct DM test statistics\n",
    "    def autocovariance(Xi, N, k, Xs):\n",
    "        autoCov = 0\n",
    "        T = float(N)\n",
    "        for i in np.arange(0, N-k):\n",
    "              autoCov += ((Xi[i+k])-Xs)*(Xi[i]-Xs)\n",
    "        return (1/(T))*autoCov\n",
    "    gamma = []\n",
    "    for lag in range(0,h):\n",
    "        gamma.append(autocovariance(d_lst,len(d_lst),lag,mean_d)) # 0, 1, 2\n",
    "    V_d = (gamma[0] + 2*sum(gamma[1:]))/T\n",
    "    DM_stat=V_d**(-0.5)*mean_d\n",
    "    harvey_adj=((T+1-2*h+h*(h-1)/T)/T)**(0.5)\n",
    "    DM_stat = harvey_adj*DM_stat\n",
    "    # Find p-value\n",
    "    p_value = 2*t.cdf(-abs(DM_stat), df = T - 1)\n",
    "    \n",
    "    # Construct named tuple for return\n",
    "    dm_return = collections.namedtuple('dm_return', 'DM p_value')\n",
    "    \n",
    "    rt = dm_return(DM = DM_stat, p_value = p_value)\n",
    "    \n",
    "    return rt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-financing",
   "metadata": {},
   "source": [
    "### Exclude Locations with Too Much Missing Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "authentic-lightweight",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = pd.read_csv('stackfootfall.csv')\n",
    "ff.timestamp = pd.to_datetime(ff.timestamp, format = '%Y-%m-%d %H:%M:%S') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "white-passion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>location</th>\n",
       "      <th>device</th>\n",
       "      <th>footfall</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>footfall_x</th>\n",
       "      <th>footfall_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1344</td>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1780.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.892256</td>\n",
       "      <td>0.833766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1345</td>\n",
       "      <td>2018-01-01 01:00:00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1780.0</td>\n",
       "      <td>899.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.892256</td>\n",
       "      <td>0.833766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1346</td>\n",
       "      <td>2018-01-01 02:00:00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1780.0</td>\n",
       "      <td>770.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.892256</td>\n",
       "      <td>0.833766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1347</td>\n",
       "      <td>2018-01-01 03:00:00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1780.0</td>\n",
       "      <td>599.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.892256</td>\n",
       "      <td>0.833766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1348</td>\n",
       "      <td>2018-01-01 04:00:00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1780.0</td>\n",
       "      <td>331.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.892256</td>\n",
       "      <td>0.833766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5002195</th>\n",
       "      <td>233635</td>\n",
       "      <td>2019-08-18 19:00:00</td>\n",
       "      <td>1168.0</td>\n",
       "      <td>827.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>2019</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>0.619529</td>\n",
       "      <td>0.955844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5002196</th>\n",
       "      <td>233636</td>\n",
       "      <td>2019-08-18 20:00:00</td>\n",
       "      <td>1168.0</td>\n",
       "      <td>827.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2019</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>0.619529</td>\n",
       "      <td>0.955844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5002197</th>\n",
       "      <td>233637</td>\n",
       "      <td>2019-08-18 21:00:00</td>\n",
       "      <td>1168.0</td>\n",
       "      <td>827.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2019</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>0.619529</td>\n",
       "      <td>0.955844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5002198</th>\n",
       "      <td>233638</td>\n",
       "      <td>2019-08-18 22:00:00</td>\n",
       "      <td>1168.0</td>\n",
       "      <td>827.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>0.619529</td>\n",
       "      <td>0.955844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5002199</th>\n",
       "      <td>233639</td>\n",
       "      <td>2019-08-18 23:00:00</td>\n",
       "      <td>1168.0</td>\n",
       "      <td>827.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2019</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>0.619529</td>\n",
       "      <td>0.955844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5002200 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0           timestamp  location  device  footfall  year  \\\n",
       "0              1344 2018-01-01 00:00:00       5.0  1780.0     494.0  2018   \n",
       "1              1345 2018-01-01 01:00:00       5.0  1780.0     899.0  2018   \n",
       "2              1346 2018-01-01 02:00:00       5.0  1780.0     770.0  2018   \n",
       "3              1347 2018-01-01 03:00:00       5.0  1780.0     599.0  2018   \n",
       "4              1348 2018-01-01 04:00:00       5.0  1780.0     331.0  2018   \n",
       "...             ...                 ...       ...     ...       ...   ...   \n",
       "5002195      233635 2019-08-18 19:00:00    1168.0   827.0      42.0  2019   \n",
       "5002196      233636 2019-08-18 20:00:00    1168.0   827.0      10.0  2019   \n",
       "5002197      233637 2019-08-18 21:00:00    1168.0   827.0      13.0  2019   \n",
       "5002198      233638 2019-08-18 22:00:00    1168.0   827.0       1.0  2019   \n",
       "5002199      233639 2019-08-18 23:00:00    1168.0   827.0       6.0  2019   \n",
       "\n",
       "         month  day  hour  day_of_week  footfall_x  footfall_y  \n",
       "0            1    1     0            1    0.892256    0.833766  \n",
       "1            1    1     1            1    0.892256    0.833766  \n",
       "2            1    1     2            1    0.892256    0.833766  \n",
       "3            1    1     3            1    0.892256    0.833766  \n",
       "4            1    1     4            1    0.892256    0.833766  \n",
       "...        ...  ...   ...          ...         ...         ...  \n",
       "5002195      8   18    19            7    0.619529    0.955844  \n",
       "5002196      8   18    20            7    0.619529    0.955844  \n",
       "5002197      8   18    21            7    0.619529    0.955844  \n",
       "5002198      8   18    22            7    0.619529    0.955844  \n",
       "5002199      8   18    23            7    0.619529    0.955844  \n",
       "\n",
       "[5002200 rows x 12 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff = ff.loc[(ff.timestamp >= '2018-01') & (ff.timestamp <= '2019-08-18 23:00')]\n",
    "\n",
    "ff_count = ff.groupby('location')['footfall'].count() / max(ff.groupby('location')['footfall'].count())\n",
    "limit1 = ff_count.loc[ff_count >= 0.6].to_frame()\n",
    "ff_test = ff.loc[(ff.timestamp >= '2018-07-29') & (ff.timestamp <= '2019-08-18 23:00')]\n",
    "ff_test_count = ff_test.groupby('location')['footfall'].count() / max(ff_test.groupby('location')['footfall'].count())\n",
    "limit2 = ff_test_count.loc[ff_test_count >= 0.8].to_frame()\n",
    "limit = pd.merge(limit1, limit2, how = 'inner', on = 'location')\n",
    "\n",
    "ff = pd.merge(ff, limit, how = 'inner', on = 'location')\n",
    "ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "appropriate-powder",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_accuracy = pd.DataFrame(columns = ('location', 'Naive', 'Moving_Average', 'Expoential_Smoothing', \n",
    "                                        'SARIMA', 'LSTM'))\n",
    "\n",
    "dmtest = pd.DataFrame(columns = ('location', 'nm', 'p_nm', 'ne', 'p_ne', 'ns', 'p_ns', \n",
    "                                 'nl', 'p_nl', 'me', 'p_me', 'ms', 'p_ms', 'ml', 'p_ml', \n",
    "                                 'es', 'p_es', 'el', 'p_el', 'sl', 'p_sl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "sharing-thesis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   5.,    6.,    7.,   31.,   62.,   78.,   87.,   88.,   89.,\n",
       "         91.,   92.,   97.,  108.,  111.,  113.,  114.,  115.,  116.,\n",
       "        117.,  119.,  121.,  123.,  125.,  126.,  127.,  130.,  137.,\n",
       "        138.,  145.,  146.,  152.,  155.,  164.,  165.,  178.,  185.,\n",
       "        186.,  188.,  190.,  218.,  222.,  233.,  237.,  249.,  251.,\n",
       "        255.,  266.,  268.,  271.,  272.,  273.,  281.,  300.,  305.,\n",
       "        306.,  313.,  321.,  324.,  329.,  334.,  357.,  376.,  382.,\n",
       "        383.,  385.,  388.,  389.,  390.,  393.,  409.,  411.,  414.,\n",
       "        421.,  422.,  425.,  428.,  429.,  430.,  434.,  436.,  437.,\n",
       "        439.,  440.,  441.,  443.,  445.,  448.,  450.,  452.,  463.,\n",
       "        464.,  480.,  488.,  489.,  490.,  491.,  495.,  498.,  500.,\n",
       "        501.,  502.,  503.,  509.,  510.,  511.,  512.,  514.,  515.,\n",
       "        516.,  517.,  519.,  521.,  522.,  523.,  524.,  526.,  527.,\n",
       "        528.,  530.,  531.,  536.,  537.,  541.,  542.,  544.,  546.,\n",
       "        547.,  548.,  552.,  554.,  556.,  559.,  560.,  562.,  563.,\n",
       "        565.,  568.,  569.,  573.,  574.,  575.,  576.,  579.,  581.,\n",
       "        583.,  584.,  585.,  587.,  589.,  591.,  592.,  593.,  597.,\n",
       "        598.,  599.,  605.,  607.,  609.,  610.,  611.,  612.,  614.,\n",
       "        618.,  620.,  621.,  639.,  642.,  643.,  646.,  648.,  649.,\n",
       "        657.,  660.,  705.,  709.,  710.,  711.,  712.,  713.,  714.,\n",
       "        715.,  716.,  717.,  719.,  722.,  724.,  726.,  729.,  734.,\n",
       "        736.,  737.,  738.,  759.,  761.,  778.,  780.,  781.,  782.,\n",
       "        783.,  789.,  790.,  792.,  793.,  796.,  799.,  801.,  808.,\n",
       "        810.,  811.,  813.,  814.,  816.,  817.,  818.,  819.,  822.,\n",
       "        824.,  825.,  829.,  831.,  832.,  835.,  838.,  839.,  841.,\n",
       "        845.,  847.,  848.,  850.,  851.,  852.,  853.,  854.,  855.,\n",
       "        856.,  857.,  858.,  859.,  860.,  861.,  863.,  871.,  872.,\n",
       "        873.,  875.,  877.,  879.,  880.,  881.,  882.,  888.,  889.,\n",
       "        890.,  891.,  892.,  895.,  896.,  897.,  899.,  900.,  901.,\n",
       "        902.,  905.,  906.,  907.,  909.,  910.,  919.,  921.,  924.,\n",
       "        926.,  927.,  928.,  929.,  930.,  933.,  934.,  937.,  938.,\n",
       "        940.,  941.,  943.,  944.,  946.,  948.,  949.,  950.,  951.,\n",
       "        952.,  955.,  957.,  958.,  959.,  960.,  961.,  962.,  963.,\n",
       "        964.,  965.,  966.,  967.,  969.,  970.,  972.,  973.,  974.,\n",
       "        979.,  981.,  982.,  983.,  984.,  986.,  988.,  989.,  994.,\n",
       "        995.,  996., 1011., 1012., 1013., 1014., 1015., 1016., 1017.,\n",
       "       1018., 1020., 1021., 1024., 1026., 1030., 1038., 1042., 1043.,\n",
       "       1044., 1046., 1050., 1051., 1052., 1053., 1054., 1056., 1057.,\n",
       "       1060., 1062., 1063., 1064., 1068., 1071., 1079., 1086., 1087.,\n",
       "       1088., 1089., 1095., 1059., 1096., 1097., 1109., 1110., 1112.,\n",
       "       1113., 1115., 1116.,  721.,  920., 1105., 1122., 1123., 1124.,\n",
       "       1128., 1135., 1136., 1138., 1139., 1141., 1145., 1146.,  564.,\n",
       "       1106., 1107., 1129., 1137., 1147., 1149., 1150., 1117., 1155.,\n",
       "       1157., 1158., 1159., 1160., 1161., 1162., 1164., 1165., 1119.,\n",
       "       1166., 1167., 1168.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff.location.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "confirmed-pantyhose",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\urbsim\\lib\\site-packages\\statsmodels\\base\\model.py:568: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "388/388 [==============================] - 10s 25ms/step - loss: 0.2320 - val_loss: 0.1322\n",
      "Epoch 2/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0694 - val_loss: 0.0955\n",
      "Epoch 3/30\n",
      "388/388 [==============================] - 8s 19ms/step - loss: 0.0482 - val_loss: 0.0763\n",
      "Epoch 4/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0369 - val_loss: 0.0629\n",
      "Epoch 5/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0330 - val_loss: 0.0548\n",
      "Epoch 6/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0299 - val_loss: 0.0556\n",
      "Epoch 7/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0282 - val_loss: 0.0536\n",
      "Epoch 8/30\n",
      "388/388 [==============================] - 8s 19ms/step - loss: 0.0272 - val_loss: 0.0609\n",
      "Epoch 9/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0255 - val_loss: 0.0580\n",
      "Epoch 10/30\n",
      "388/388 [==============================] - 8s 19ms/step - loss: 0.0255 - val_loss: 0.0525\n",
      "Epoch 11/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0242 - val_loss: 0.0526\n",
      "Epoch 12/30\n",
      "388/388 [==============================] - 8s 19ms/step - loss: 0.0230 - val_loss: 0.0529\n",
      "Epoch 13/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0225 - val_loss: 0.0576\n",
      "Epoch 14/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0229 - val_loss: 0.0534\n",
      "Epoch 15/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0226 - val_loss: 0.0548\n",
      "Epoch 16/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0222 - val_loss: 0.0589\n",
      "Epoch 17/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0222 - val_loss: 0.0588\n",
      "Epoch 18/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0211 - val_loss: 0.0540\n",
      "Epoch 19/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0202 - val_loss: 0.0605\n",
      "Epoch 20/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0204 - val_loss: 0.0531\n",
      "Epoch 21/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0207 - val_loss: 0.0511\n",
      "Epoch 22/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0202 - val_loss: 0.0567\n",
      "Epoch 23/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0198 - val_loss: 0.0525\n",
      "Epoch 24/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0196 - val_loss: 0.0573\n",
      "Epoch 25/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0191 - val_loss: 0.0456\n",
      "Epoch 26/30\n",
      "388/388 [==============================] - 8s 21ms/step - loss: 0.0184 - val_loss: 0.0486\n",
      "Epoch 27/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0178 - val_loss: 0.0516\n",
      "Epoch 28/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0188 - val_loss: 0.0483\n",
      "Epoch 29/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0185 - val_loss: 0.0503\n",
      "Epoch 30/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0193 - val_loss: 0.0553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\urbsim\\lib\\site-packages\\statsmodels\\base\\model.py:568: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "384/384 [==============================] - 8s 21ms/step - loss: 0.2974 - val_loss: 0.1757\n",
      "Epoch 2/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.1358 - val_loss: 0.1515\n",
      "Epoch 3/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0904 - val_loss: 0.1054\n",
      "Epoch 4/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0723 - val_loss: 0.1048\n",
      "Epoch 5/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0652 - val_loss: 0.1091\n",
      "Epoch 6/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0632 - val_loss: 0.0958\n",
      "Epoch 7/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0595 - val_loss: 0.1001\n",
      "Epoch 8/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0585 - val_loss: 0.0935\n",
      "Epoch 9/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0574 - val_loss: 0.1013\n",
      "Epoch 10/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0557 - val_loss: 0.0976\n",
      "Epoch 11/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0554 - val_loss: 0.0923\n",
      "Epoch 12/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0521 - val_loss: 0.0861\n",
      "Epoch 13/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0528 - val_loss: 0.0888\n",
      "Epoch 14/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0515 - val_loss: 0.0895\n",
      "Epoch 15/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0501 - val_loss: 0.0840\n",
      "Epoch 16/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0505 - val_loss: 0.0806\n",
      "Epoch 17/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0501 - val_loss: 0.0847\n",
      "Epoch 18/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0488 - val_loss: 0.0804\n",
      "Epoch 19/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0489 - val_loss: 0.0780\n",
      "Epoch 20/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0473 - val_loss: 0.0771\n",
      "Epoch 21/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0476 - val_loss: 0.0803\n",
      "Epoch 22/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0450 - val_loss: 0.0803\n",
      "Epoch 23/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0444 - val_loss: 0.0748\n",
      "Epoch 24/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0439 - val_loss: 0.0790\n",
      "Epoch 25/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0433 - val_loss: 0.0756\n",
      "Epoch 26/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0442 - val_loss: 0.0773\n",
      "Epoch 27/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0423 - val_loss: 0.0722\n",
      "Epoch 28/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0425 - val_loss: 0.0742\n",
      "Epoch 29/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0419 - val_loss: 0.0769\n",
      "Epoch 30/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0417 - val_loss: 0.0791\n",
      "Epoch 1/30\n",
      "387/387 [==============================] - 8s 21ms/step - loss: 0.3455 - val_loss: 0.1880\n",
      "Epoch 2/30\n",
      "387/387 [==============================] - 7s 19ms/step - loss: 0.1363 - val_loss: 0.1224\n",
      "Epoch 3/30\n",
      "387/387 [==============================] - 7s 19ms/step - loss: 0.0943 - val_loss: 0.1016\n",
      "Epoch 4/30\n",
      "387/387 [==============================] - 7s 19ms/step - loss: 0.0820 - val_loss: 0.1085\n",
      "Epoch 5/30\n",
      "387/387 [==============================] - 7s 19ms/step - loss: 0.0762 - val_loss: 0.1096\n",
      "Epoch 6/30\n",
      "387/387 [==============================] - 7s 19ms/step - loss: 0.0719 - val_loss: 0.0958\n",
      "Epoch 7/30\n",
      "387/387 [==============================] - 8s 20ms/step - loss: 0.0697 - val_loss: 0.0837\n",
      "Epoch 8/30\n",
      "387/387 [==============================] - 7s 19ms/step - loss: 0.0674 - val_loss: 0.0819\n",
      "Epoch 9/30\n",
      "387/387 [==============================] - 7s 19ms/step - loss: 0.0645 - val_loss: 0.0788\n",
      "Epoch 10/30\n",
      "387/387 [==============================] - 7s 19ms/step - loss: 0.0640 - val_loss: 0.0758\n",
      "Epoch 11/30\n",
      "387/387 [==============================] - 7s 19ms/step - loss: 0.0615 - val_loss: 0.0734\n",
      "Epoch 12/30\n",
      "387/387 [==============================] - 8s 19ms/step - loss: 0.0596 - val_loss: 0.0717\n",
      "Epoch 13/30\n",
      "387/387 [==============================] - 7s 19ms/step - loss: 0.0578 - val_loss: 0.0673\n",
      "Epoch 14/30\n",
      "387/387 [==============================] - 7s 19ms/step - loss: 0.0567 - val_loss: 0.0702\n",
      "Epoch 15/30\n",
      "387/387 [==============================] - 8s 20ms/step - loss: 0.0568 - val_loss: 0.0653\n",
      "Epoch 16/30\n",
      "387/387 [==============================] - 7s 19ms/step - loss: 0.0546 - val_loss: 0.0656\n",
      "Epoch 17/30\n",
      "387/387 [==============================] - 7s 19ms/step - loss: 0.0535 - val_loss: 0.0694\n",
      "Epoch 18/30\n",
      "387/387 [==============================] - 7s 19ms/step - loss: 0.0542 - val_loss: 0.0635\n",
      "Epoch 19/30\n",
      "387/387 [==============================] - 7s 19ms/step - loss: 0.0511 - val_loss: 0.0629\n",
      "Epoch 20/30\n",
      "387/387 [==============================] - 7s 19ms/step - loss: 0.0503 - val_loss: 0.0621\n",
      "Epoch 21/30\n",
      "387/387 [==============================] - 7s 19ms/step - loss: 0.0482 - val_loss: 0.0565\n",
      "Epoch 22/30\n",
      "387/387 [==============================] - 8s 20ms/step - loss: 0.0480 - val_loss: 0.0615\n",
      "Epoch 23/30\n",
      "387/387 [==============================] - 8s 20ms/step - loss: 0.0464 - val_loss: 0.0604\n",
      "Epoch 24/30\n",
      "387/387 [==============================] - 8s 19ms/step - loss: 0.0459 - val_loss: 0.0580\n",
      "Epoch 25/30\n",
      "387/387 [==============================] - 8s 20ms/step - loss: 0.0447 - val_loss: 0.0557\n",
      "Epoch 26/30\n",
      "387/387 [==============================] - 8s 19ms/step - loss: 0.0448 - val_loss: 0.0552\n",
      "Epoch 27/30\n",
      "387/387 [==============================] - 7s 19ms/step - loss: 0.0431 - val_loss: 0.0549\n",
      "Epoch 28/30\n",
      "387/387 [==============================] - 8s 19ms/step - loss: 0.0427 - val_loss: 0.0559\n",
      "Epoch 29/30\n",
      "387/387 [==============================] - 8s 19ms/step - loss: 0.0432 - val_loss: 0.0548\n",
      "Epoch 30/30\n",
      "387/387 [==============================] - 7s 19ms/step - loss: 0.0418 - val_loss: 0.0497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\urbsim\\lib\\site-packages\\statsmodels\\base\\model.py:568: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "388/388 [==============================] - 8s 21ms/step - loss: 0.2240 - val_loss: 0.1180\n",
      "Epoch 2/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0880 - val_loss: 0.0620\n",
      "Epoch 3/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0598 - val_loss: 0.0471\n",
      "Epoch 4/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0487 - val_loss: 0.0425\n",
      "Epoch 5/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0431 - val_loss: 0.0424\n",
      "Epoch 6/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0400 - val_loss: 0.0427\n",
      "Epoch 7/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0382 - val_loss: 0.0396\n",
      "Epoch 8/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0374 - val_loss: 0.0369\n",
      "Epoch 9/30\n",
      "388/388 [==============================] - 8s 19ms/step - loss: 0.0360 - val_loss: 0.0429\n",
      "Epoch 10/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0350 - val_loss: 0.0396\n",
      "Epoch 11/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0337 - val_loss: 0.0401\n",
      "Epoch 12/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0331 - val_loss: 0.0433\n",
      "Epoch 13/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0328 - val_loss: 0.0425\n",
      "Epoch 14/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0318 - val_loss: 0.0400\n",
      "Epoch 15/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0308 - val_loss: 0.0442\n",
      "Epoch 16/30\n",
      "388/388 [==============================] - 8s 19ms/step - loss: 0.0299 - val_loss: 0.0423\n",
      "Epoch 17/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0291 - val_loss: 0.0432\n",
      "Epoch 18/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0289 - val_loss: 0.0461\n",
      "Epoch 19/30\n",
      "388/388 [==============================] - 8s 19ms/step - loss: 0.0285 - val_loss: 0.0439\n",
      "Epoch 20/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0281 - val_loss: 0.0431\n",
      "Epoch 21/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0276 - val_loss: 0.0428\n",
      "Epoch 22/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0276 - val_loss: 0.0421\n",
      "Epoch 23/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0279 - val_loss: 0.0418\n",
      "Epoch 24/30\n",
      "388/388 [==============================] - 8s 19ms/step - loss: 0.0271 - val_loss: 0.0412\n",
      "Epoch 25/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0269 - val_loss: 0.0387\n",
      "Epoch 26/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0269 - val_loss: 0.0389\n",
      "Epoch 27/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0275 - val_loss: 0.0358\n",
      "Epoch 28/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0265 - val_loss: 0.0351\n",
      "Epoch 29/30\n",
      "388/388 [==============================] - 8s 19ms/step - loss: 0.0272 - val_loss: 0.0359\n",
      "Epoch 30/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0270 - val_loss: 0.0358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\urbsim\\lib\\site-packages\\statsmodels\\base\\model.py:568: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "388/388 [==============================] - 8s 21ms/step - loss: 0.2734 - val_loss: 0.1960\n",
      "Epoch 2/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.1239 - val_loss: 0.0874\n",
      "Epoch 3/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0702 - val_loss: 0.0868\n",
      "Epoch 4/30\n",
      "388/388 [==============================] - 8s 19ms/step - loss: 0.0582 - val_loss: 0.0808\n",
      "Epoch 5/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0531 - val_loss: 0.0733\n",
      "Epoch 6/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0496 - val_loss: 0.0720\n",
      "Epoch 7/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0469 - val_loss: 0.0719\n",
      "Epoch 8/30\n",
      "388/388 [==============================] - 8s 19ms/step - loss: 0.0474 - val_loss: 0.0667\n",
      "Epoch 9/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0458 - val_loss: 0.0671\n",
      "Epoch 10/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0433 - val_loss: 0.0678\n",
      "Epoch 11/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0428 - val_loss: 0.0639\n",
      "Epoch 12/30\n",
      "388/388 [==============================] - 8s 19ms/step - loss: 0.0408 - val_loss: 0.0706\n",
      "Epoch 13/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0408 - val_loss: 0.0664\n",
      "Epoch 14/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0401 - val_loss: 0.0646\n",
      "Epoch 15/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0392 - val_loss: 0.0637\n",
      "Epoch 16/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0377 - val_loss: 0.0625\n",
      "Epoch 17/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0374 - val_loss: 0.0629\n",
      "Epoch 18/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0363 - val_loss: 0.0604\n",
      "Epoch 19/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0354 - val_loss: 0.0605\n",
      "Epoch 20/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0352 - val_loss: 0.0594\n",
      "Epoch 21/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0347 - val_loss: 0.0609\n",
      "Epoch 22/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0340 - val_loss: 0.0568\n",
      "Epoch 23/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0339 - val_loss: 0.0594\n",
      "Epoch 24/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0334 - val_loss: 0.0561\n",
      "Epoch 25/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0328 - val_loss: 0.0576\n",
      "Epoch 26/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0326 - val_loss: 0.0581\n",
      "Epoch 27/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0320 - val_loss: 0.0565\n",
      "Epoch 28/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0319 - val_loss: 0.0572\n",
      "Epoch 29/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0307 - val_loss: 0.0565\n",
      "Epoch 30/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0308 - val_loss: 0.0588\n",
      "Epoch 1/30\n",
      "388/388 [==============================] - 9s 22ms/step - loss: 0.2307 - val_loss: 0.1370\n",
      "Epoch 2/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.1145 - val_loss: 0.1055\n",
      "Epoch 3/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0912 - val_loss: 0.0898\n",
      "Epoch 4/30\n",
      "388/388 [==============================] - 8s 21ms/step - loss: 0.0762 - val_loss: 0.0759\n",
      "Epoch 5/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0682 - val_loss: 0.0709\n",
      "Epoch 6/30\n",
      "388/388 [==============================] - 8s 21ms/step - loss: 0.0639 - val_loss: 0.0662\n",
      "Epoch 7/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0599 - val_loss: 0.0612\n",
      "Epoch 8/30\n",
      "388/388 [==============================] - 8s 21ms/step - loss: 0.0576 - val_loss: 0.0614\n",
      "Epoch 9/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0557 - val_loss: 0.0603\n",
      "Epoch 10/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0538 - val_loss: 0.0572\n",
      "Epoch 11/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0506 - val_loss: 0.0590\n",
      "Epoch 12/30\n",
      "388/388 [==============================] - 8s 21ms/step - loss: 0.0524 - val_loss: 0.0546\n",
      "Epoch 13/30\n",
      "388/388 [==============================] - 8s 21ms/step - loss: 0.0490 - val_loss: 0.0554\n",
      "Epoch 14/30\n",
      "388/388 [==============================] - 8s 21ms/step - loss: 0.0470 - val_loss: 0.0556\n",
      "Epoch 15/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0458 - val_loss: 0.0564\n",
      "Epoch 16/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0444 - val_loss: 0.0532\n",
      "Epoch 17/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0444 - val_loss: 0.0476\n",
      "Epoch 18/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0436 - val_loss: 0.0513\n",
      "Epoch 19/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0434 - val_loss: 0.0510\n",
      "Epoch 20/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0433 - val_loss: 0.0469\n",
      "Epoch 21/30\n",
      "388/388 [==============================] - 8s 21ms/step - loss: 0.0412 - val_loss: 0.0479\n",
      "Epoch 22/30\n",
      "388/388 [==============================] - 8s 21ms/step - loss: 0.0413 - val_loss: 0.0449\n",
      "Epoch 23/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0404 - val_loss: 0.0446\n",
      "Epoch 24/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0406 - val_loss: 0.0525\n",
      "Epoch 25/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0397 - val_loss: 0.0452\n",
      "Epoch 26/30\n",
      "388/388 [==============================] - 8s 21ms/step - loss: 0.0399 - val_loss: 0.0464\n",
      "Epoch 27/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0387 - val_loss: 0.0448\n",
      "Epoch 28/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0390 - val_loss: 0.0420\n",
      "Epoch 29/30\n",
      "388/388 [==============================] - 8s 21ms/step - loss: 0.0404 - val_loss: 0.0446\n",
      "Epoch 30/30\n",
      "388/388 [==============================] - 8s 21ms/step - loss: 0.0394 - val_loss: 0.0433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\urbsim\\lib\\site-packages\\statsmodels\\base\\model.py:568: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "388/388 [==============================] - 9s 22ms/step - loss: 0.2476 - val_loss: 0.2659\n",
      "Epoch 2/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0945 - val_loss: 0.1422\n",
      "Epoch 3/30\n",
      "388/388 [==============================] - 8s 21ms/step - loss: 0.0702 - val_loss: 0.0943\n",
      "Epoch 4/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0584 - val_loss: 0.0746\n",
      "Epoch 5/30\n",
      "388/388 [==============================] - 8s 21ms/step - loss: 0.0487 - val_loss: 0.0579\n",
      "Epoch 6/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0441 - val_loss: 0.0501\n",
      "Epoch 7/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0393 - val_loss: 0.0407\n",
      "Epoch 8/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0344 - val_loss: 0.0398\n",
      "Epoch 9/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0300 - val_loss: 0.0364\n",
      "Epoch 10/30\n",
      "388/388 [==============================] - 8s 21ms/step - loss: 0.0293 - val_loss: 0.0344\n",
      "Epoch 11/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0268 - val_loss: 0.0336\n",
      "Epoch 12/30\n",
      "388/388 [==============================] - 8s 21ms/step - loss: 0.0257 - val_loss: 0.0326\n",
      "Epoch 13/30\n",
      "388/388 [==============================] - 8s 21ms/step - loss: 0.0251 - val_loss: 0.0328\n",
      "Epoch 14/30\n",
      "388/388 [==============================] - 8s 21ms/step - loss: 0.0245 - val_loss: 0.0293\n",
      "Epoch 15/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0229 - val_loss: 0.0280\n",
      "Epoch 16/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0220 - val_loss: 0.0283\n",
      "Epoch 17/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0226 - val_loss: 0.0262\n",
      "Epoch 18/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0214 - val_loss: 0.0254\n",
      "Epoch 19/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0216 - val_loss: 0.0253\n",
      "Epoch 20/30\n",
      "388/388 [==============================] - 8s 21ms/step - loss: 0.0215 - val_loss: 0.0234\n",
      "Epoch 21/30\n",
      "388/388 [==============================] - 8s 21ms/step - loss: 0.0210 - val_loss: 0.0231\n",
      "Epoch 22/30\n",
      "388/388 [==============================] - 8s 21ms/step - loss: 0.0202 - val_loss: 0.0230\n",
      "Epoch 23/30\n",
      "388/388 [==============================] - 8s 21ms/step - loss: 0.0200 - val_loss: 0.0229\n",
      "Epoch 24/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0200 - val_loss: 0.0233\n",
      "Epoch 25/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0194 - val_loss: 0.0221\n",
      "Epoch 26/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0197 - val_loss: 0.0229\n",
      "Epoch 27/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0194 - val_loss: 0.0230\n",
      "Epoch 28/30\n",
      "388/388 [==============================] - 8s 21ms/step - loss: 0.0191 - val_loss: 0.0219\n",
      "Epoch 29/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0190 - val_loss: 0.0220\n",
      "Epoch 30/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0186 - val_loss: 0.0224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\urbsim\\lib\\site-packages\\statsmodels\\base\\model.py:568: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "388/388 [==============================] - 8s 22ms/step - loss: 0.2388 - val_loss: 0.1338\n",
      "Epoch 2/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.1057 - val_loss: 0.0935\n",
      "Epoch 3/30\n",
      "388/388 [==============================] - 8s 21ms/step - loss: 0.0730 - val_loss: 0.0654\n",
      "Epoch 4/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0553 - val_loss: 0.0465\n",
      "Epoch 5/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0446 - val_loss: 0.0378\n",
      "Epoch 6/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0371 - val_loss: 0.0361\n",
      "Epoch 7/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0332 - val_loss: 0.0325\n",
      "Epoch 8/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0299 - val_loss: 0.0358\n",
      "Epoch 9/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0275 - val_loss: 0.0374\n",
      "Epoch 10/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0271 - val_loss: 0.0359\n",
      "Epoch 11/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0253 - val_loss: 0.0432\n",
      "Epoch 12/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0252 - val_loss: 0.0453\n",
      "Epoch 13/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0243 - val_loss: 0.0387\n",
      "Epoch 14/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0242 - val_loss: 0.0464\n",
      "Epoch 15/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0234 - val_loss: 0.0427\n",
      "Epoch 16/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0240 - val_loss: 0.0582\n",
      "Epoch 17/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0238 - val_loss: 0.0426\n",
      "Epoch 18/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0236 - val_loss: 0.0520\n",
      "Epoch 19/30\n",
      "388/388 [==============================] - 8s 19ms/step - loss: 0.0234 - val_loss: 0.0452\n",
      "Epoch 20/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0230 - val_loss: 0.0437\n",
      "Epoch 21/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0223 - val_loss: 0.0441\n",
      "Epoch 22/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0224 - val_loss: 0.0333\n",
      "Epoch 23/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0222 - val_loss: 0.0464\n",
      "Epoch 24/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0227 - val_loss: 0.0366\n",
      "Epoch 25/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0221 - val_loss: 0.0375\n",
      "Epoch 26/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0214 - val_loss: 0.0378\n",
      "Epoch 27/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0217 - val_loss: 0.0416\n",
      "Epoch 28/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0215 - val_loss: 0.0446\n",
      "Epoch 29/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0214 - val_loss: 0.0386\n",
      "Epoch 30/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0217 - val_loss: 0.0388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\urbsim\\lib\\site-packages\\statsmodels\\base\\model.py:568: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "388/388 [==============================] - 8s 21ms/step - loss: 0.2986 - val_loss: 0.3434\n",
      "Epoch 2/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.1400 - val_loss: 0.2170\n",
      "Epoch 3/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0931 - val_loss: 0.1719\n",
      "Epoch 4/30\n",
      "388/388 [==============================] - 8s 19ms/step - loss: 0.0811 - val_loss: 0.1905\n",
      "Epoch 5/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0779 - val_loss: 0.1620\n",
      "Epoch 6/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0749 - val_loss: 0.1518\n",
      "Epoch 7/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0714 - val_loss: 0.1391\n",
      "Epoch 8/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0708 - val_loss: 0.1370\n",
      "Epoch 9/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0680 - val_loss: 0.1243\n",
      "Epoch 10/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0667 - val_loss: 0.1187\n",
      "Epoch 11/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0651 - val_loss: 0.1131\n",
      "Epoch 12/30\n",
      "388/388 [==============================] - 8s 19ms/step - loss: 0.0643 - val_loss: 0.1079\n",
      "Epoch 13/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0615 - val_loss: 0.0983\n",
      "Epoch 14/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0604 - val_loss: 0.1034\n",
      "Epoch 15/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0599 - val_loss: 0.1003\n",
      "Epoch 16/30\n",
      "388/388 [==============================] - 8s 19ms/step - loss: 0.0593 - val_loss: 0.0998\n",
      "Epoch 17/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0585 - val_loss: 0.0993\n",
      "Epoch 18/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0579 - val_loss: 0.0923\n",
      "Epoch 19/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0566 - val_loss: 0.0906\n",
      "Epoch 20/30\n",
      "388/388 [==============================] - 8s 19ms/step - loss: 0.0559 - val_loss: 0.0869\n",
      "Epoch 21/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0556 - val_loss: 0.0825\n",
      "Epoch 22/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0540 - val_loss: 0.0791\n",
      "Epoch 23/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0536 - val_loss: 0.0785\n",
      "Epoch 24/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0533 - val_loss: 0.0779\n",
      "Epoch 25/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0528 - val_loss: 0.0734\n",
      "Epoch 26/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0523 - val_loss: 0.0696\n",
      "Epoch 27/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0514 - val_loss: 0.0711\n",
      "Epoch 28/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0517 - val_loss: 0.0705\n",
      "Epoch 29/30\n",
      "388/388 [==============================] - 8s 19ms/step - loss: 0.0513 - val_loss: 0.0696\n",
      "Epoch 30/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0511 - val_loss: 0.0721\n",
      "Epoch 1/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.3303 - val_loss: 0.2379\n",
      "Epoch 2/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.1936 - val_loss: 0.2154\n",
      "Epoch 3/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.1575 - val_loss: 0.1683\n",
      "Epoch 4/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.1312 - val_loss: 0.1531\n",
      "Epoch 5/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.1095 - val_loss: 0.1160\n",
      "Epoch 6/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0913 - val_loss: 0.1089\n",
      "Epoch 7/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0840 - val_loss: 0.0983\n",
      "Epoch 8/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0795 - val_loss: 0.0954\n",
      "Epoch 9/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0746 - val_loss: 0.0932\n",
      "Epoch 10/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0712 - val_loss: 0.0906\n",
      "Epoch 11/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0679 - val_loss: 0.0879\n",
      "Epoch 12/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0675 - val_loss: 0.0875\n",
      "Epoch 13/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0655 - val_loss: 0.0892\n",
      "Epoch 14/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0644 - val_loss: 0.0881\n",
      "Epoch 15/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0634 - val_loss: 0.0860\n",
      "Epoch 16/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0614 - val_loss: 0.0903\n",
      "Epoch 17/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0611 - val_loss: 0.0838\n",
      "Epoch 18/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0595 - val_loss: 0.0832\n",
      "Epoch 19/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0578 - val_loss: 0.0781\n",
      "Epoch 20/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0583 - val_loss: 0.0755\n",
      "Epoch 21/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0578 - val_loss: 0.0799\n",
      "Epoch 22/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0570 - val_loss: 0.0778\n",
      "Epoch 23/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0582 - val_loss: 0.0823\n",
      "Epoch 24/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0571 - val_loss: 0.0748\n",
      "Epoch 25/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0547 - val_loss: 0.0694\n",
      "Epoch 26/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0550 - val_loss: 0.0722\n",
      "Epoch 27/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0547 - val_loss: 0.0664\n",
      "Epoch 28/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0534 - val_loss: 0.0662\n",
      "Epoch 29/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0529 - val_loss: 0.0655\n",
      "Epoch 30/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0526 - val_loss: 0.0662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\urbsim\\lib\\site-packages\\statsmodels\\base\\model.py:568: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.3875 - val_loss: 0.3347\n",
      "Epoch 2/30\n",
      "388/388 [==============================] - 7s 18ms/step - loss: 0.1807 - val_loss: 0.4411\n",
      "Epoch 3/30\n",
      "388/388 [==============================] - 7s 17ms/step - loss: 0.1070 - val_loss: 0.2383\n",
      "Epoch 4/30\n",
      "388/388 [==============================] - 7s 18ms/step - loss: 0.0832 - val_loss: 0.1794\n",
      "Epoch 5/30\n",
      "388/388 [==============================] - 8s 21ms/step - loss: 0.0769 - val_loss: 0.1739\n",
      "Epoch 6/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0715 - val_loss: 0.1572\n",
      "Epoch 7/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0671 - val_loss: 0.1586\n",
      "Epoch 8/30\n",
      "388/388 [==============================] - 7s 18ms/step - loss: 0.0648 - val_loss: 0.1398\n",
      "Epoch 9/30\n",
      "388/388 [==============================] - 7s 18ms/step - loss: 0.0607 - val_loss: 0.1414\n",
      "Epoch 10/30\n",
      "388/388 [==============================] - 7s 18ms/step - loss: 0.0598 - val_loss: 0.1485\n",
      "Epoch 11/30\n",
      "388/388 [==============================] - 7s 18ms/step - loss: 0.0568 - val_loss: 0.1503\n",
      "Epoch 12/30\n",
      "388/388 [==============================] - 7s 18ms/step - loss: 0.0544 - val_loss: 0.1142\n",
      "Epoch 13/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0522 - val_loss: 0.1000\n",
      "Epoch 14/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0514 - val_loss: 0.1332\n",
      "Epoch 15/30\n",
      "388/388 [==============================] - 7s 18ms/step - loss: 0.0500 - val_loss: 0.0965\n",
      "Epoch 16/30\n",
      "388/388 [==============================] - 7s 18ms/step - loss: 0.0484 - val_loss: 0.1014\n",
      "Epoch 17/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0459 - val_loss: 0.1118\n",
      "Epoch 18/30\n",
      "388/388 [==============================] - 7s 18ms/step - loss: 0.0472 - val_loss: 0.1227\n",
      "Epoch 19/30\n",
      "388/388 [==============================] - 7s 18ms/step - loss: 0.0468 - val_loss: 0.1033\n",
      "Epoch 20/30\n",
      "388/388 [==============================] - 7s 18ms/step - loss: 0.0452 - val_loss: 0.1039\n",
      "Epoch 21/30\n",
      "388/388 [==============================] - 7s 18ms/step - loss: 0.0443 - val_loss: 0.1049\n",
      "Epoch 22/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0434 - val_loss: 0.1022\n",
      "Epoch 23/30\n",
      "388/388 [==============================] - 7s 18ms/step - loss: 0.0433 - val_loss: 0.0948\n",
      "Epoch 24/30\n",
      "388/388 [==============================] - 7s 18ms/step - loss: 0.0419 - val_loss: 0.0773\n",
      "Epoch 25/30\n",
      "388/388 [==============================] - 7s 18ms/step - loss: 0.0416 - val_loss: 0.1013\n",
      "Epoch 26/30\n",
      "388/388 [==============================] - 7s 18ms/step - loss: 0.0404 - val_loss: 0.0899\n",
      "Epoch 27/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0403 - val_loss: 0.0947\n",
      "Epoch 28/30\n",
      "388/388 [==============================] - 7s 18ms/step - loss: 0.0408 - val_loss: 0.0865\n",
      "Epoch 29/30\n",
      "388/388 [==============================] - 7s 18ms/step - loss: 0.0406 - val_loss: 0.0989\n",
      "Epoch 30/30\n",
      "388/388 [==============================] - 7s 18ms/step - loss: 0.0398 - val_loss: 0.0836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\urbsim\\lib\\site-packages\\statsmodels\\base\\model.py:568: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "388/388 [==============================] - 8s 21ms/step - loss: 0.2150 - val_loss: 0.1153\n",
      "Epoch 2/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0935 - val_loss: 0.0774\n",
      "Epoch 3/30\n",
      "388/388 [==============================] - 8s 19ms/step - loss: 0.0639 - val_loss: 0.0501\n",
      "Epoch 4/30\n",
      "388/388 [==============================] - 8s 19ms/step - loss: 0.0502 - val_loss: 0.0468\n",
      "Epoch 5/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0458 - val_loss: 0.0436\n",
      "Epoch 6/30\n",
      "388/388 [==============================] - 8s 19ms/step - loss: 0.0443 - val_loss: 0.0482\n",
      "Epoch 7/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0434 - val_loss: 0.0468\n",
      "Epoch 8/30\n",
      "388/388 [==============================] - 8s 19ms/step - loss: 0.0401 - val_loss: 0.0445\n",
      "Epoch 9/30\n",
      "388/388 [==============================] - 8s 19ms/step - loss: 0.0401 - val_loss: 0.0465\n",
      "Epoch 10/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0394 - val_loss: 0.0452\n",
      "Epoch 11/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0376 - val_loss: 0.0448\n",
      "Epoch 12/30\n",
      "388/388 [==============================] - 8s 19ms/step - loss: 0.0359 - val_loss: 0.0452\n",
      "Epoch 13/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0349 - val_loss: 0.0456\n",
      "Epoch 14/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0330 - val_loss: 0.0377\n",
      "Epoch 15/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0327 - val_loss: 0.0436\n",
      "Epoch 16/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0312 - val_loss: 0.0395\n",
      "Epoch 17/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0305 - val_loss: 0.0404\n",
      "Epoch 18/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0301 - val_loss: 0.0450\n",
      "Epoch 19/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0295 - val_loss: 0.0379\n",
      "Epoch 20/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0288 - val_loss: 0.0386\n",
      "Epoch 21/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0287 - val_loss: 0.0378\n",
      "Epoch 22/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0276 - val_loss: 0.0412\n",
      "Epoch 23/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0282 - val_loss: 0.0373\n",
      "Epoch 24/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0280 - val_loss: 0.0415\n",
      "Epoch 25/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0272 - val_loss: 0.0383\n",
      "Epoch 26/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0278 - val_loss: 0.0404\n",
      "Epoch 27/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0271 - val_loss: 0.0434\n",
      "Epoch 28/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0263 - val_loss: 0.0350\n",
      "Epoch 29/30\n",
      "388/388 [==============================] - 8s 19ms/step - loss: 0.0257 - val_loss: 0.0399\n",
      "Epoch 30/30\n",
      "388/388 [==============================] - 8s 19ms/step - loss: 0.0258 - val_loss: 0.0386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\urbsim\\lib\\site-packages\\statsmodels\\base\\model.py:568: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "388/388 [==============================] - 8s 21ms/step - loss: 0.1872 - val_loss: 0.0831\n",
      "Epoch 2/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0736 - val_loss: 0.0576\n",
      "Epoch 3/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0463 - val_loss: 0.0465\n",
      "Epoch 4/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0347 - val_loss: 0.0419\n",
      "Epoch 5/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0283 - val_loss: 0.0392\n",
      "Epoch 6/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0264 - val_loss: 0.0328\n",
      "Epoch 7/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0241 - val_loss: 0.0287\n",
      "Epoch 8/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0220 - val_loss: 0.0324\n",
      "Epoch 9/30\n",
      "388/388 [==============================] - 8s 19ms/step - loss: 0.0215 - val_loss: 0.0305\n",
      "Epoch 10/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0203 - val_loss: 0.0304\n",
      "Epoch 11/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0195 - val_loss: 0.0302\n",
      "Epoch 12/30\n",
      "388/388 [==============================] - 8s 19ms/step - loss: 0.0207 - val_loss: 0.0298\n",
      "Epoch 13/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0201 - val_loss: 0.0302\n",
      "Epoch 14/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0185 - val_loss: 0.0296\n",
      "Epoch 15/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0182 - val_loss: 0.0319\n",
      "Epoch 16/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0181 - val_loss: 0.0297\n",
      "Epoch 17/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0176 - val_loss: 0.0323\n",
      "Epoch 18/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0172 - val_loss: 0.0289\n",
      "Epoch 19/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0174 - val_loss: 0.0263\n",
      "Epoch 20/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0167 - val_loss: 0.0281\n",
      "Epoch 21/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0165 - val_loss: 0.0305\n",
      "Epoch 22/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0166 - val_loss: 0.0295\n",
      "Epoch 23/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0159 - val_loss: 0.0253\n",
      "Epoch 24/30\n",
      "388/388 [==============================] - 8s 19ms/step - loss: 0.0159 - val_loss: 0.0238\n",
      "Epoch 25/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0155 - val_loss: 0.0256\n",
      "Epoch 26/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0155 - val_loss: 0.0248\n",
      "Epoch 27/30\n",
      "388/388 [==============================] - 8s 19ms/step - loss: 0.0154 - val_loss: 0.0235\n",
      "Epoch 28/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0151 - val_loss: 0.0225\n",
      "Epoch 29/30\n",
      "388/388 [==============================] - 8s 19ms/step - loss: 0.0149 - val_loss: 0.0231\n",
      "Epoch 30/30\n",
      "388/388 [==============================] - 8s 19ms/step - loss: 0.0152 - val_loss: 0.0245\n",
      "Epoch 1/30\n",
      "388/388 [==============================] - 8s 21ms/step - loss: 0.8896 - val_loss: 0.1029\n",
      "Epoch 2/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.7103 - val_loss: 0.1121\n",
      "Epoch 3/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.7212 - val_loss: 0.0989\n",
      "Epoch 4/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.5719 - val_loss: 0.0988\n",
      "Epoch 5/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.4823 - val_loss: 0.0853\n",
      "Epoch 6/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.2278 - val_loss: 0.0803\n",
      "Epoch 7/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.1661 - val_loss: 0.0667\n",
      "Epoch 8/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.2271 - val_loss: 0.0699\n",
      "Epoch 9/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.2200 - val_loss: 0.0676\n",
      "Epoch 10/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.1471 - val_loss: 0.0621\n",
      "Epoch 11/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.1233 - val_loss: 0.0575\n",
      "Epoch 12/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.1140 - val_loss: 0.0564\n",
      "Epoch 13/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.1099 - val_loss: 0.0545\n",
      "Epoch 14/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.1034 - val_loss: 0.0561\n",
      "Epoch 15/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0944 - val_loss: 0.0556\n",
      "Epoch 16/30\n",
      "388/388 [==============================] - 8s 19ms/step - loss: 0.0961 - val_loss: 0.0567\n",
      "Epoch 17/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0883 - val_loss: 0.0546\n",
      "Epoch 18/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0881 - val_loss: 0.0535\n",
      "Epoch 19/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0813 - val_loss: 0.0549\n",
      "Epoch 20/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0750 - val_loss: 0.0532\n",
      "Epoch 21/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0673 - val_loss: 0.0545\n",
      "Epoch 22/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0703 - val_loss: 0.0550\n",
      "Epoch 23/30\n",
      "388/388 [==============================] - 8s 19ms/step - loss: 0.0588 - val_loss: 0.0603\n",
      "Epoch 24/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0526 - val_loss: 0.0572\n",
      "Epoch 25/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0623 - val_loss: 0.0522\n",
      "Epoch 26/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0558 - val_loss: 0.0540\n",
      "Epoch 27/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0682 - val_loss: 0.0539\n",
      "Epoch 28/30\n",
      "388/388 [==============================] - 7s 19ms/step - loss: 0.0779 - val_loss: 0.0551\n",
      "Epoch 29/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0862 - val_loss: 0.0567\n",
      "Epoch 30/30\n",
      "388/388 [==============================] - 8s 20ms/step - loss: 0.0641 - val_loss: 0.0526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\urbsim\\lib\\site-packages\\statsmodels\\base\\model.py:568: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "384/384 [==============================] - 8s 21ms/step - loss: 0.1773 - val_loss: 0.0893\n",
      "Epoch 2/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0882 - val_loss: 0.0823\n",
      "Epoch 3/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0519 - val_loss: 0.0501\n",
      "Epoch 4/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0356 - val_loss: 0.0568\n",
      "Epoch 5/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0314 - val_loss: 0.0425\n",
      "Epoch 6/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0280 - val_loss: 0.0436\n",
      "Epoch 7/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0264 - val_loss: 0.0438\n",
      "Epoch 8/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0253 - val_loss: 0.0424\n",
      "Epoch 9/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0242 - val_loss: 0.0406\n",
      "Epoch 10/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0233 - val_loss: 0.0428\n",
      "Epoch 11/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0229 - val_loss: 0.0413\n",
      "Epoch 12/30\n",
      "384/384 [==============================] - 8s 20ms/step - loss: 0.0220 - val_loss: 0.0390\n",
      "Epoch 13/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0213 - val_loss: 0.0385\n",
      "Epoch 14/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0198 - val_loss: 0.0414\n",
      "Epoch 15/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0204 - val_loss: 0.0419\n",
      "Epoch 16/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0196 - val_loss: 0.0416\n",
      "Epoch 17/30\n",
      "384/384 [==============================] - 8s 20ms/step - loss: 0.0185 - val_loss: 0.0400\n",
      "Epoch 18/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0179 - val_loss: 0.0384\n",
      "Epoch 19/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0169 - val_loss: 0.0395\n",
      "Epoch 20/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0160 - val_loss: 0.0411\n",
      "Epoch 21/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0161 - val_loss: 0.0357\n",
      "Epoch 22/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0156 - val_loss: 0.0390\n",
      "Epoch 23/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0149 - val_loss: 0.0405\n",
      "Epoch 24/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0150 - val_loss: 0.0380\n",
      "Epoch 25/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0148 - val_loss: 0.0373\n",
      "Epoch 26/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0147 - val_loss: 0.0404\n",
      "Epoch 27/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0142 - val_loss: 0.0385\n",
      "Epoch 28/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0143 - val_loss: 0.0357\n",
      "Epoch 29/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0137 - val_loss: 0.0366\n",
      "Epoch 30/30\n",
      "384/384 [==============================] - 7s 19ms/step - loss: 0.0139 - val_loss: 0.0376\n"
     ]
    }
   ],
   "source": [
    "for loc in ff.location.unique()[150:170]:\n",
    "\n",
    "    ff_loc = ff.loc[(ff.location <= loc) & (ff.location >= loc)][[\n",
    "        'timestamp','footfall','year','month','day','hour','day_of_week']]    \n",
    "    ff_loc.index = ff_loc.timestamp\n",
    "    ff_loc = ff_loc.resample('H').mean()\n",
    "\n",
    "    ff_loc['footfall'].replace(0, np.nan, inplace = True)\n",
    "    ff_loc['footfall'] = ff_loc['footfall'].interpolate(method='linear')\n",
    "    ff_loc = ff_loc.reset_index(level = ['timestamp'])\n",
    "    ff_loc.timestamp = pd.to_datetime(ff_loc.timestamp, format = '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    temp = ff_loc[['timestamp', 'footfall']]\n",
    "    temp.index = temp.timestamp\n",
    "    temp['year'] = temp.timestamp.dt.year\n",
    "    temp['month'] = temp.timestamp.dt.month\n",
    "    temp['day'] = temp.timestamp.dt.day\n",
    "    temp['hour'] = temp.timestamp.dt.hour\n",
    "    temp['day_of_week'] = temp.timestamp.dt.dayofweek + 1\n",
    "\n",
    "    Train = temp.loc[(temp.timestamp >= '2018-01-01 00:00') & (temp.timestamp <= '2019-07-28 23:00')]\n",
    "    Valid = temp.loc[(temp.timestamp >= '2019-07-29 00:00') & (temp.timestamp <= '2019-08-18 23:00')]\n",
    "    \n",
    "    if len(Valid) > 0:\n",
    "        train = Train.resample('D').mean()\n",
    "        valid = Valid.resample('D').mean() \n",
    "\n",
    "        train['footfall'] = train['footfall'].interpolate(method='linear')\n",
    "        valid['footfall'] = valid['footfall'].interpolate(method='linear')\n",
    "\n",
    "        # Monday to Thursday Ratio\n",
    "        mtt = Train.loc[(Train.day_of_week >= 1) & (Train.day_of_week <= 4)].groupby('hour')['footfall'].mean().to_frame()\n",
    "        mtt['ratio'] = mtt['footfall']/mtt['footfall'].sum()\n",
    "        mtt = mtt.reset_index(level = ['hour'])\n",
    "\n",
    "        # Friday Ratio\n",
    "        fri = Train.loc[(Train.day_of_week >= 5) & (Train.day_of_week <= 5)].groupby('hour')['footfall'].mean().to_frame()\n",
    "        fri['ratio'] = fri['footfall']/fri['footfall'].sum()\n",
    "        fri = fri.reset_index(level = ['hour'])\n",
    "\n",
    "        # Sat Ratio\n",
    "        sat = Train.loc[(Train.day_of_week >= 6) & (Train.day_of_week <= 6)].groupby('hour')['footfall'].mean().to_frame()\n",
    "        sat['ratio'] = sat['footfall']/sat['footfall'].sum()\n",
    "        sat = sat.reset_index(level = ['hour'])\n",
    "\n",
    "        # Sun Ratio\n",
    "        sun = Train.loc[(Train.day_of_week >= 7) & (Train.day_of_week <= 7)].groupby('hour')['footfall'].mean().to_frame()\n",
    "        sun['ratio'] = sun['footfall']/sun['footfall'].sum()\n",
    "        sun = sun.reset_index(level = ['hour'])\n",
    "\n",
    "        ratio1 = mtt[['hour','ratio']]\n",
    "        ratio1['day_of_week'] = 1\n",
    "        ratio2 = mtt[['hour','ratio']]\n",
    "        ratio2['day_of_week'] = 2\n",
    "        ratio3 = mtt[['hour','ratio']]\n",
    "        ratio3['day_of_week'] = 3\n",
    "        ratio4 = mtt[['hour','ratio']]\n",
    "        ratio4['day_of_week'] = 4\n",
    "        ratio5 = fri[['hour','ratio']] \n",
    "        ratio5['day_of_week'] = 5\n",
    "        ratio6 = sat[['hour','ratio']]\n",
    "        ratio6['day_of_week'] = 6\n",
    "        ratio7 = sun[['hour','ratio']]\n",
    "        ratio7['day_of_week'] = 7\n",
    "\n",
    "        ratio_week = ratio1.append(ratio2).append(ratio3).append(ratio4).append(ratio5).append(ratio6).append(ratio7)\n",
    "\n",
    "        merge = pd.merge(Valid, ratio_week, on = ('hour','day_of_week'), how = 'left')\n",
    "\n",
    "        # Naive Approach\n",
    "        dd = np.asarray(Train['footfall'])\n",
    "        y_hat = Valid.copy()\n",
    "        y_hat['naive']= dd[len(dd)- 1]\n",
    "        for i in range(len(Valid)): \n",
    "            y_hat['naive'][i]= dd[len(dd) - len(Valid) + i]\n",
    "        naive_rmse = rmse(Valid.footfall, y_hat.naive)\n",
    "\n",
    "\n",
    "        # Moving Average Approach    \n",
    "        moving_avg(4)\n",
    "        avg4_rmse = rmse(Valid.footfall, y_hat.avg)\n",
    "        moving_avg(6)\n",
    "        avg6_rmse = rmse(Valid.footfall, y_hat.avg)    \n",
    "        moving_avg(8)\n",
    "        avg8_rmse = rmse(Valid.footfall, y_hat.avg)\n",
    "        moving_avg(10)\n",
    "        avg10_rmse = rmse(Valid.footfall, y_hat.avg)\n",
    "\n",
    "        avg_rmse = min(avg4_rmse, avg6_rmse, avg8_rmse, avg10_rmse) \n",
    "        ma_temp = [avg4_rmse, avg6_rmse, avg8_rmse, avg10_rmse] \n",
    "        windows = (ma_temp.index(avg_rmse) + 2) * 2   \n",
    "        moving_avg(windows)\n",
    "\n",
    "\n",
    "        # Exponential Smoothing\n",
    "        y_hat2 = valid.copy()\n",
    "        fit1 = ExponentialSmoothing(np.asarray(train.footfall), seasonal_periods = 7, trend = 'add', seasonal= 'add').fit()\n",
    "        y_hat2['Holt_Winter'] = fit1.forecast(len(valid))\n",
    "\n",
    "        pred_d = y_hat2.Holt_Winter.to_frame()\n",
    "        pred_d = pred_d.reset_index(level = ['timestamp'])\n",
    "        pred_d['month'] = pred_d.timestamp.dt.month\n",
    "        pred_d['day'] = pred_d.timestamp.dt.day\n",
    "        temp1 = pd.merge(merge, pred_d, on = ('month','day'), how = 'left')\n",
    "        temp1['prediction'] = temp1['ratio'] * temp1['Holt_Winter'] * 24\n",
    "        temp1.index = temp1.timestamp_x\n",
    "        y_hat['Holt_Winter'] = temp1['prediction']\n",
    "        holt_winter_rmse = rmse(Valid.footfall, y_hat['Holt_Winter'])\n",
    "\n",
    "        # SARIMA\n",
    "        fit1 = sm.tsa.statespace.SARIMAX(train.footfall, order = (1,1,3), seasonal_order =(1,1,3,7)).fit()\n",
    "        y_hat2['SARIMA'] = fit1.predict(start = \"2019-07-29 00:00\", end = \"2019-08-18 23:00\", dynamic=True)\n",
    "\n",
    "        pred_d = y_hat2['SARIMA'].to_frame()\n",
    "        pred_d = pred_d.reset_index(level = ['timestamp'])\n",
    "        pred_d['month'] = pred_d.timestamp.dt.month\n",
    "        pred_d['day'] = pred_d.timestamp.dt.day\n",
    "        temp2 = pd.merge(merge, pred_d, on = ('month','day'), how = 'left')\n",
    "        temp2['prediction'] = temp2['ratio'] * temp2['SARIMA'] * 24\n",
    "        temp2.index = temp2.timestamp_x\n",
    "        y_hat['SARIMA'] = temp2['prediction']\n",
    "        sarima_rmse = rmse(Valid.footfall, y_hat['SARIMA'])\n",
    "\n",
    "        # LSTM \n",
    "        train = temp.loc[(temp.timestamp >= '2018-01-01 00:00') & (temp.timestamp <= '2019-07-28 23:00')][[\n",
    "            'footfall','month','day','hour','day_of_week']]\n",
    "        test = temp.loc[(temp.timestamp >= '2019-07-28 14:00') & (temp.timestamp <= '2019-08-18 23:00')][[\n",
    "            'footfall','month','day','hour','day_of_week']]  \n",
    "\n",
    "        ff_transformer = RobustScaler()\n",
    "        ff_transformer = ff_transformer.fit(train[['footfall']])\n",
    "\n",
    "        train['footfall'] = ff_transformer.transform(train[['footfall']])\n",
    "        test['footfall'] = ff_transformer.transform(test[['footfall']])\n",
    "\n",
    "        time_steps = 10\n",
    "        X_train, y_train = create_dataset(train, train.footfall, time_steps)\n",
    "        X_test, y_test = create_dataset(test, test.footfall, time_steps)\n",
    "\n",
    "        model = keras.Sequential()\n",
    "        model.add(\n",
    "          keras.layers.Bidirectional(\n",
    "            keras.layers.LSTM(\n",
    "              units=128, \n",
    "              input_shape=(X_train.shape[1], X_train.shape[2])\n",
    "            )\n",
    "          )\n",
    "        )\n",
    "        model.add(keras.layers.Dropout(rate=0.2))\n",
    "        model.add(keras.layers.Dense(units=1))\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')    \n",
    "\n",
    "        history = model.fit(\n",
    "        X_train, y_train, \n",
    "        epochs = 30, \n",
    "        batch_size = 32, \n",
    "        validation_split = 0.1,\n",
    "        shuffle = False\n",
    "        )\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        y_train_inv = ff_transformer.inverse_transform(y_train.reshape(1, -1))\n",
    "        y_test_inv = ff_transformer.inverse_transform(y_test.reshape(1, -1))\n",
    "        y_pred_inv = ff_transformer.inverse_transform(y_pred)\n",
    "\n",
    "        y_hat['LSTM'] =  y_pred_inv.flatten()\n",
    "        lstm_rmse = rmse(y_test_inv.flatten(), y_pred_inv.flatten())\n",
    "\n",
    "        # save prediction\n",
    "        pred_loc = y_hat[['naive','avg','Holt_Winter','SARIMA','LSTM']]\n",
    "        pred_loc.to_csv( 'pred/'+ str(loc) + '.csv')\n",
    "\n",
    "        # DM-Test\n",
    "        dm_NM = dm_test(Valid.footfall, y_hat['naive'], y_hat['avg'], h = 1, crit=\"MSE\")\n",
    "        dm_NE = dm_test(Valid.footfall, y_hat['naive'], y_hat['Holt_Winter'], h = 1, crit=\"MSE\")    \n",
    "        dm_NS = dm_test(Valid.footfall, y_hat['naive'], y_hat['SARIMA'], h = 1, crit=\"MSE\")  \n",
    "        dm_NL = dm_test(Valid.footfall, y_hat['naive'], y_pred_inv.flatten(), h = 1, crit=\"MSE\")   \n",
    "        dm_ME = dm_test(Valid.footfall, y_hat['avg'], y_hat['Holt_Winter'], h = 1, crit=\"MSE\")   \n",
    "        dm_MS = dm_test(Valid.footfall, y_hat['avg'], y_hat['SARIMA'], h = 1, crit=\"MSE\")   \n",
    "        dm_ML = dm_test(Valid.footfall, y_hat['avg'],  y_pred_inv.flatten(), h = 1, crit=\"MSE\")   \n",
    "        dm_ES = dm_test(Valid.footfall, y_hat['Holt_Winter'], y_hat['SARIMA'], h = 1, crit=\"MSE\")    \n",
    "        dm_EL = dm_test(Valid.footfall, y_hat['Holt_Winter'], y_pred_inv.flatten(), h = 1, crit=\"MSE\")  \n",
    "        dm_SL = dm_test(Valid.footfall, y_hat['SARIMA'], y_pred_inv.flatten(), h = 1, crit=\"MSE\")\n",
    "\n",
    "        dmtest = dmtest.append({'location': loc, 'nm': dm_NM.DM, 'p_nm': dm_NM.p_value, \n",
    "                                'ne': dm_NE.DM, 'p_ne': dm_NE.p_value, 'ns': dm_NS.DM, 'p_ns': dm_NS.p_value, \n",
    "                                'nl': dm_NL.DM, 'p_nl': dm_NL.p_value, 'me': dm_ME.DM, 'p_me': dm_ME.p_value, \n",
    "                                'ms': dm_MS.DM, 'p_ms': dm_MS.p_value, 'ml': dm_ML.DM, 'p_ml': dm_ML.p_value, \n",
    "                                'es': dm_ES.DM, 'p_es': dm_ES.p_value, 'el': dm_EL.DM, 'p_el': dm_EL.p_value, \n",
    "                                'sl': dm_SL.DM, 'p_sl': dm_SL.p_value}, ignore_index=True)\n",
    "\n",
    "\n",
    "        pred_accuracy = pred_accuracy.append({'location': loc, 'Naive': naive_rmse, 'Moving_Average': avg_rmse, \n",
    "                                            'Expoential_Smoothing': holt_winter_rmse, 'SARIMA': sarima_rmse, \n",
    "                                            'LSTM': lstm_rmse}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "given-discipline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>Naive</th>\n",
       "      <th>Moving_Average</th>\n",
       "      <th>Expoential_Smoothing</th>\n",
       "      <th>SARIMA</th>\n",
       "      <th>LSTM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>649.0</td>\n",
       "      <td>152.446447</td>\n",
       "      <td>126.736308</td>\n",
       "      <td>155.839159</td>\n",
       "      <td>123.081832</td>\n",
       "      <td>119.733967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>657.0</td>\n",
       "      <td>115.470451</td>\n",
       "      <td>117.050864</td>\n",
       "      <td>149.063380</td>\n",
       "      <td>128.964391</td>\n",
       "      <td>90.084942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>660.0</td>\n",
       "      <td>121.518827</td>\n",
       "      <td>98.962515</td>\n",
       "      <td>139.913709</td>\n",
       "      <td>136.558083</td>\n",
       "      <td>89.530249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>705.0</td>\n",
       "      <td>271.650142</td>\n",
       "      <td>210.870070</td>\n",
       "      <td>251.154268</td>\n",
       "      <td>226.985080</td>\n",
       "      <td>184.235458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>709.0</td>\n",
       "      <td>953.031099</td>\n",
       "      <td>718.172147</td>\n",
       "      <td>659.167248</td>\n",
       "      <td>1785.860576</td>\n",
       "      <td>442.999469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>618.0</td>\n",
       "      <td>293.503782</td>\n",
       "      <td>293.328260</td>\n",
       "      <td>381.767447</td>\n",
       "      <td>266.996647</td>\n",
       "      <td>165.791184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>620.0</td>\n",
       "      <td>84.110713</td>\n",
       "      <td>82.344358</td>\n",
       "      <td>73.814893</td>\n",
       "      <td>87.262827</td>\n",
       "      <td>87.529517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>639.0</td>\n",
       "      <td>68.578061</td>\n",
       "      <td>53.407148</td>\n",
       "      <td>54.006851</td>\n",
       "      <td>55.602200</td>\n",
       "      <td>49.279187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>642.0</td>\n",
       "      <td>43.010888</td>\n",
       "      <td>36.685520</td>\n",
       "      <td>55.225211</td>\n",
       "      <td>51.503534</td>\n",
       "      <td>31.890926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>648.0</td>\n",
       "      <td>290.569972</td>\n",
       "      <td>289.826229</td>\n",
       "      <td>232.287999</td>\n",
       "      <td>234.701426</td>\n",
       "      <td>119.988219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>207 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     location       Naive  Moving_Average  Expoential_Smoothing       SARIMA  \\\n",
       "0       649.0  152.446447      126.736308            155.839159   123.081832   \n",
       "1       657.0  115.470451      117.050864            149.063380   128.964391   \n",
       "2       660.0  121.518827       98.962515            139.913709   136.558083   \n",
       "3       705.0  271.650142      210.870070            251.154268   226.985080   \n",
       "4       709.0  953.031099      718.172147            659.167248  1785.860576   \n",
       "..        ...         ...             ...                   ...          ...   \n",
       "202     618.0  293.503782      293.328260            381.767447   266.996647   \n",
       "203     620.0   84.110713       82.344358             73.814893    87.262827   \n",
       "204     639.0   68.578061       53.407148             54.006851    55.602200   \n",
       "205     642.0   43.010888       36.685520             55.225211    51.503534   \n",
       "206     648.0  290.569972      289.826229            232.287999   234.701426   \n",
       "\n",
       "           LSTM  \n",
       "0    119.733967  \n",
       "1     90.084942  \n",
       "2     89.530249  \n",
       "3    184.235458  \n",
       "4    442.999469  \n",
       "..          ...  \n",
       "202  165.791184  \n",
       "203   87.529517  \n",
       "204   49.279187  \n",
       "205   31.890926  \n",
       "206  119.988219  \n",
       "\n",
       "[207 rows x 6 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_accuracy = pred_accuracy.drop_duplicates(subset='location')\n",
    "pred_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "muslim-scoop",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>nm</th>\n",
       "      <th>p_nm</th>\n",
       "      <th>ne</th>\n",
       "      <th>p_ne</th>\n",
       "      <th>ns</th>\n",
       "      <th>p_ns</th>\n",
       "      <th>nl</th>\n",
       "      <th>p_nl</th>\n",
       "      <th>me</th>\n",
       "      <th>...</th>\n",
       "      <th>ms</th>\n",
       "      <th>p_ms</th>\n",
       "      <th>ml</th>\n",
       "      <th>p_ml</th>\n",
       "      <th>es</th>\n",
       "      <th>p_es</th>\n",
       "      <th>el</th>\n",
       "      <th>p_el</th>\n",
       "      <th>sl</th>\n",
       "      <th>p_sl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>649.0</td>\n",
       "      <td>3.546045</td>\n",
       "      <td>4.276080e-04</td>\n",
       "      <td>-0.408963</td>\n",
       "      <td>6.827407e-01</td>\n",
       "      <td>4.093780</td>\n",
       "      <td>4.943738e-05</td>\n",
       "      <td>3.966777</td>\n",
       "      <td>8.343337e-05</td>\n",
       "      <td>-5.142984</td>\n",
       "      <td>...</td>\n",
       "      <td>1.593227</td>\n",
       "      <td>1.117374e-01</td>\n",
       "      <td>1.036569</td>\n",
       "      <td>3.004347e-01</td>\n",
       "      <td>6.100975</td>\n",
       "      <td>2.105414e-09</td>\n",
       "      <td>4.826646</td>\n",
       "      <td>1.844483e-06</td>\n",
       "      <td>0.518004</td>\n",
       "      <td>6.046832e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>657.0</td>\n",
       "      <td>-0.275268</td>\n",
       "      <td>7.832231e-01</td>\n",
       "      <td>-4.652335</td>\n",
       "      <td>4.201824e-06</td>\n",
       "      <td>-2.602655</td>\n",
       "      <td>9.523463e-03</td>\n",
       "      <td>3.533407</td>\n",
       "      <td>4.480571e-04</td>\n",
       "      <td>-4.447895</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.760637</td>\n",
       "      <td>5.979560e-03</td>\n",
       "      <td>3.911994</td>\n",
       "      <td>1.041156e-04</td>\n",
       "      <td>4.479601</td>\n",
       "      <td>9.264363e-06</td>\n",
       "      <td>5.988023</td>\n",
       "      <td>4.045203e-09</td>\n",
       "      <td>5.844916</td>\n",
       "      <td>9.120505e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>660.0</td>\n",
       "      <td>2.136547</td>\n",
       "      <td>3.311676e-02</td>\n",
       "      <td>-1.871715</td>\n",
       "      <td>6.182624e-02</td>\n",
       "      <td>-1.547532</td>\n",
       "      <td>1.223640e-01</td>\n",
       "      <td>2.824611</td>\n",
       "      <td>4.921779e-03</td>\n",
       "      <td>-11.416242</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.893467</td>\n",
       "      <td>5.907835e-25</td>\n",
       "      <td>2.534696</td>\n",
       "      <td>1.155654e-02</td>\n",
       "      <td>2.743397</td>\n",
       "      <td>6.297782e-03</td>\n",
       "      <td>12.156039</td>\n",
       "      <td>5.462983e-30</td>\n",
       "      <td>12.626319</td>\n",
       "      <td>6.112249e-32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>705.0</td>\n",
       "      <td>4.126811</td>\n",
       "      <td>4.304850e-05</td>\n",
       "      <td>1.297692</td>\n",
       "      <td>1.949879e-01</td>\n",
       "      <td>2.859547</td>\n",
       "      <td>4.418702e-03</td>\n",
       "      <td>4.979523</td>\n",
       "      <td>8.774295e-07</td>\n",
       "      <td>-6.109131</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.521727</td>\n",
       "      <td>4.677639e-04</td>\n",
       "      <td>3.671329</td>\n",
       "      <td>2.671043e-04</td>\n",
       "      <td>4.854340</td>\n",
       "      <td>1.614548e-06</td>\n",
       "      <td>6.625135</td>\n",
       "      <td>8.940581e-11</td>\n",
       "      <td>5.660855</td>\n",
       "      <td>2.533942e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>709.0</td>\n",
       "      <td>2.343784</td>\n",
       "      <td>2.809796e-02</td>\n",
       "      <td>2.281848</td>\n",
       "      <td>3.207584e-02</td>\n",
       "      <td>-4.923383</td>\n",
       "      <td>5.640781e-05</td>\n",
       "      <td>2.991030</td>\n",
       "      <td>6.526270e-03</td>\n",
       "      <td>1.126822</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.583782</td>\n",
       "      <td>1.109549e-05</td>\n",
       "      <td>2.430890</td>\n",
       "      <td>2.326723e-02</td>\n",
       "      <td>-5.604969</td>\n",
       "      <td>1.053732e-05</td>\n",
       "      <td>2.327757</td>\n",
       "      <td>2.908144e-02</td>\n",
       "      <td>6.325615</td>\n",
       "      <td>1.870059e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>618.0</td>\n",
       "      <td>0.018056</td>\n",
       "      <td>9.856015e-01</td>\n",
       "      <td>-6.101760</td>\n",
       "      <td>2.095810e-09</td>\n",
       "      <td>2.121741</td>\n",
       "      <td>3.434773e-02</td>\n",
       "      <td>7.533519</td>\n",
       "      <td>2.310444e-13</td>\n",
       "      <td>-9.723995</td>\n",
       "      <td>...</td>\n",
       "      <td>4.526586</td>\n",
       "      <td>7.490291e-06</td>\n",
       "      <td>9.465891</td>\n",
       "      <td>1.123787e-19</td>\n",
       "      <td>13.401975</td>\n",
       "      <td>3.075686e-35</td>\n",
       "      <td>12.386070</td>\n",
       "      <td>6.135109e-31</td>\n",
       "      <td>7.623441</td>\n",
       "      <td>1.240526e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>620.0</td>\n",
       "      <td>0.420978</td>\n",
       "      <td>6.739513e-01</td>\n",
       "      <td>2.706819</td>\n",
       "      <td>7.024085e-03</td>\n",
       "      <td>-0.677611</td>\n",
       "      <td>4.983300e-01</td>\n",
       "      <td>-0.734189</td>\n",
       "      <td>4.631754e-01</td>\n",
       "      <td>3.162039</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.018286</td>\n",
       "      <td>4.409179e-02</td>\n",
       "      <td>-1.681920</td>\n",
       "      <td>9.320510e-02</td>\n",
       "      <td>-5.671796</td>\n",
       "      <td>2.386399e-08</td>\n",
       "      <td>-3.723168</td>\n",
       "      <td>2.189722e-04</td>\n",
       "      <td>-0.070340</td>\n",
       "      <td>9.439512e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>639.0</td>\n",
       "      <td>5.317610</td>\n",
       "      <td>1.584378e-07</td>\n",
       "      <td>4.435169</td>\n",
       "      <td>1.130763e-05</td>\n",
       "      <td>4.587744</td>\n",
       "      <td>5.663940e-06</td>\n",
       "      <td>5.413689</td>\n",
       "      <td>9.576277e-08</td>\n",
       "      <td>-0.407577</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.785799</td>\n",
       "      <td>7.473416e-02</td>\n",
       "      <td>1.674110</td>\n",
       "      <td>9.473061e-02</td>\n",
       "      <td>-0.903625</td>\n",
       "      <td>3.666269e-01</td>\n",
       "      <td>2.049335</td>\n",
       "      <td>4.094750e-02</td>\n",
       "      <td>2.486040</td>\n",
       "      <td>1.324088e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>642.0</td>\n",
       "      <td>2.341270</td>\n",
       "      <td>1.960777e-02</td>\n",
       "      <td>-6.545941</td>\n",
       "      <td>1.460417e-10</td>\n",
       "      <td>-4.639159</td>\n",
       "      <td>4.466977e-06</td>\n",
       "      <td>5.076409</td>\n",
       "      <td>5.424543e-07</td>\n",
       "      <td>-8.727257</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.417905</td>\n",
       "      <td>5.097337e-13</td>\n",
       "      <td>2.353490</td>\n",
       "      <td>1.898198e-02</td>\n",
       "      <td>11.985763</td>\n",
       "      <td>2.717932e-29</td>\n",
       "      <td>16.389214</td>\n",
       "      <td>1.103553e-48</td>\n",
       "      <td>15.329677</td>\n",
       "      <td>8.432123e-44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>648.0</td>\n",
       "      <td>0.112970</td>\n",
       "      <td>9.101495e-01</td>\n",
       "      <td>6.352271</td>\n",
       "      <td>1.061391e-09</td>\n",
       "      <td>6.443483</td>\n",
       "      <td>6.374988e-10</td>\n",
       "      <td>4.728394</td>\n",
       "      <td>3.869183e-06</td>\n",
       "      <td>5.682913</td>\n",
       "      <td>...</td>\n",
       "      <td>5.898298</td>\n",
       "      <td>1.246881e-08</td>\n",
       "      <td>4.263564</td>\n",
       "      <td>2.898175e-05</td>\n",
       "      <td>-1.962663</td>\n",
       "      <td>5.084546e-02</td>\n",
       "      <td>3.402498</td>\n",
       "      <td>7.827035e-04</td>\n",
       "      <td>3.376333</td>\n",
       "      <td>8.572832e-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>207 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     location        nm          p_nm        ne          p_ne        ns  \\\n",
       "0       649.0  3.546045  4.276080e-04 -0.408963  6.827407e-01  4.093780   \n",
       "1       657.0 -0.275268  7.832231e-01 -4.652335  4.201824e-06 -2.602655   \n",
       "2       660.0  2.136547  3.311676e-02 -1.871715  6.182624e-02 -1.547532   \n",
       "3       705.0  4.126811  4.304850e-05  1.297692  1.949879e-01  2.859547   \n",
       "4       709.0  2.343784  2.809796e-02  2.281848  3.207584e-02 -4.923383   \n",
       "..        ...       ...           ...       ...           ...       ...   \n",
       "202     618.0  0.018056  9.856015e-01 -6.101760  2.095810e-09  2.121741   \n",
       "203     620.0  0.420978  6.739513e-01  2.706819  7.024085e-03 -0.677611   \n",
       "204     639.0  5.317610  1.584378e-07  4.435169  1.130763e-05  4.587744   \n",
       "205     642.0  2.341270  1.960777e-02 -6.545941  1.460417e-10 -4.639159   \n",
       "206     648.0  0.112970  9.101495e-01  6.352271  1.061391e-09  6.443483   \n",
       "\n",
       "             p_ns        nl          p_nl         me  ...         ms  \\\n",
       "0    4.943738e-05  3.966777  8.343337e-05  -5.142984  ...   1.593227   \n",
       "1    9.523463e-03  3.533407  4.480571e-04  -4.447895  ...  -2.760637   \n",
       "2    1.223640e-01  2.824611  4.921779e-03 -11.416242  ... -10.893467   \n",
       "3    4.418702e-03  4.979523  8.774295e-07  -6.109131  ...  -3.521727   \n",
       "4    5.640781e-05  2.991030  6.526270e-03   1.126822  ...  -5.583782   \n",
       "..            ...       ...           ...        ...  ...        ...   \n",
       "202  3.434773e-02  7.533519  2.310444e-13  -9.723995  ...   4.526586   \n",
       "203  4.983300e-01 -0.734189  4.631754e-01   3.162039  ...  -2.018286   \n",
       "204  5.663940e-06  5.413689  9.576277e-08  -0.407577  ...  -1.785799   \n",
       "205  4.466977e-06  5.076409  5.424543e-07  -8.727257  ...  -7.417905   \n",
       "206  6.374988e-10  4.728394  3.869183e-06   5.682913  ...   5.898298   \n",
       "\n",
       "             p_ms        ml          p_ml         es          p_es         el  \\\n",
       "0    1.117374e-01  1.036569  3.004347e-01   6.100975  2.105414e-09   4.826646   \n",
       "1    5.979560e-03  3.911994  1.041156e-04   4.479601  9.264363e-06   5.988023   \n",
       "2    5.907835e-25  2.534696  1.155654e-02   2.743397  6.297782e-03  12.156039   \n",
       "3    4.677639e-04  3.671329  2.671043e-04   4.854340  1.614548e-06   6.625135   \n",
       "4    1.109549e-05  2.430890  2.326723e-02  -5.604969  1.053732e-05   2.327757   \n",
       "..            ...       ...           ...        ...           ...        ...   \n",
       "202  7.490291e-06  9.465891  1.123787e-19  13.401975  3.075686e-35  12.386070   \n",
       "203  4.409179e-02 -1.681920  9.320510e-02  -5.671796  2.386399e-08  -3.723168   \n",
       "204  7.473416e-02  1.674110  9.473061e-02  -0.903625  3.666269e-01   2.049335   \n",
       "205  5.097337e-13  2.353490  1.898198e-02  11.985763  2.717932e-29  16.389214   \n",
       "206  1.246881e-08  4.263564  2.898175e-05  -1.962663  5.084546e-02   3.402498   \n",
       "\n",
       "             p_el         sl          p_sl  \n",
       "0    1.844483e-06   0.518004  6.046832e-01  \n",
       "1    4.045203e-09   5.844916  9.120505e-09  \n",
       "2    5.462983e-30  12.626319  6.112249e-32  \n",
       "3    8.940581e-11   5.660855  2.533942e-08  \n",
       "4    2.908144e-02   6.325615  1.870059e-06  \n",
       "..            ...        ...           ...  \n",
       "202  6.135109e-31   7.623441  1.240526e-13  \n",
       "203  2.189722e-04  -0.070340  9.439512e-01  \n",
       "204  4.094750e-02   2.486040  1.324088e-02  \n",
       "205  1.103553e-48  15.329677  8.432123e-44  \n",
       "206  7.827035e-04   3.376333  8.572832e-04  \n",
       "\n",
       "[207 rows x 21 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dmtest = dmtest.drop_duplicates(subset='location')\n",
    "dmtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "continuous-willow",
   "metadata": {},
   "outputs": [],
   "source": [
    "dmtest.to_csv('dmtest2.csv')\n",
    "pred_accuracy.to_csv('pred_accuracy2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "quiet-advance",
   "metadata": {},
   "outputs": [],
   "source": [
    "dmtest1 = pd.read_csv('dmtest.csv')\n",
    "dmtest1 = dmtest1.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "pred_accuracy1 = pd.read_csv('pred_accuracy.csv')\n",
    "pred_accuracy1 = pred_accuracy1.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "available-journalist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>dm_nai_mov</th>\n",
       "      <th>p_nai_mov</th>\n",
       "      <th>dm_nai_es</th>\n",
       "      <th>p_nai_es</th>\n",
       "      <th>dm_nai_sa</th>\n",
       "      <th>p_nai_sa</th>\n",
       "      <th>dm_nai_ls</th>\n",
       "      <th>p_nai_ls</th>\n",
       "      <th>dm_mov_es</th>\n",
       "      <th>...</th>\n",
       "      <th>dm_mov_sa</th>\n",
       "      <th>p_mov_sa</th>\n",
       "      <th>dm_mov_ls</th>\n",
       "      <th>p_mov_ls</th>\n",
       "      <th>dm_es_sa</th>\n",
       "      <th>p_es_sa</th>\n",
       "      <th>dm_es_ls</th>\n",
       "      <th>p_es_ls</th>\n",
       "      <th>dm_sa_ls</th>\n",
       "      <th>p_sa_ls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.0</td>\n",
       "      <td>5.572342</td>\n",
       "      <td>4.579726e-08</td>\n",
       "      <td>6.613142</td>\n",
       "      <td>1.185217e-10</td>\n",
       "      <td>6.646138</td>\n",
       "      <td>9.687066e-11</td>\n",
       "      <td>7.527820</td>\n",
       "      <td>3.351889e-13</td>\n",
       "      <td>4.922440</td>\n",
       "      <td>...</td>\n",
       "      <td>4.777178</td>\n",
       "      <td>2.485800e-06</td>\n",
       "      <td>8.146106</td>\n",
       "      <td>4.660093e-15</td>\n",
       "      <td>-1.420115</td>\n",
       "      <td>1.563399e-01</td>\n",
       "      <td>5.066094</td>\n",
       "      <td>6.170705e-07</td>\n",
       "      <td>5.054385</td>\n",
       "      <td>6.537654e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125.0</td>\n",
       "      <td>1.854522</td>\n",
       "      <td>6.676648e-02</td>\n",
       "      <td>0.824226</td>\n",
       "      <td>4.118758e-01</td>\n",
       "      <td>0.026970</td>\n",
       "      <td>9.785399e-01</td>\n",
       "      <td>1.712714</td>\n",
       "      <td>9.002724e-02</td>\n",
       "      <td>-2.625987</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.579867</td>\n",
       "      <td>1.141633e-02</td>\n",
       "      <td>-0.097533</td>\n",
       "      <td>9.225083e-01</td>\n",
       "      <td>-1.793056</td>\n",
       "      <td>7.614635e-02</td>\n",
       "      <td>1.154491</td>\n",
       "      <td>2.511949e-01</td>\n",
       "      <td>1.532285</td>\n",
       "      <td>1.287755e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>126.0</td>\n",
       "      <td>0.748011</td>\n",
       "      <td>4.548032e-01</td>\n",
       "      <td>4.046126</td>\n",
       "      <td>6.026210e-05</td>\n",
       "      <td>2.633514</td>\n",
       "      <td>8.710786e-03</td>\n",
       "      <td>4.564412</td>\n",
       "      <td>6.303586e-06</td>\n",
       "      <td>4.807093</td>\n",
       "      <td>...</td>\n",
       "      <td>3.306822</td>\n",
       "      <td>1.011074e-03</td>\n",
       "      <td>5.678262</td>\n",
       "      <td>2.303179e-08</td>\n",
       "      <td>-4.516985</td>\n",
       "      <td>7.823993e-06</td>\n",
       "      <td>1.363633</td>\n",
       "      <td>1.732929e-01</td>\n",
       "      <td>6.390069</td>\n",
       "      <td>3.783688e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>127.0</td>\n",
       "      <td>2.001185</td>\n",
       "      <td>4.590922e-02</td>\n",
       "      <td>3.030513</td>\n",
       "      <td>2.567100e-03</td>\n",
       "      <td>3.258509</td>\n",
       "      <td>1.195720e-03</td>\n",
       "      <td>1.658482</td>\n",
       "      <td>9.784354e-02</td>\n",
       "      <td>1.311638</td>\n",
       "      <td>...</td>\n",
       "      <td>1.616946</td>\n",
       "      <td>1.065169e-01</td>\n",
       "      <td>-0.046836</td>\n",
       "      <td>9.626622e-01</td>\n",
       "      <td>2.270732</td>\n",
       "      <td>2.358519e-02</td>\n",
       "      <td>-1.263966</td>\n",
       "      <td>2.068276e-01</td>\n",
       "      <td>-1.643004</td>\n",
       "      <td>1.010071e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>145.0</td>\n",
       "      <td>-0.941103</td>\n",
       "      <td>3.471038e-01</td>\n",
       "      <td>4.766637</td>\n",
       "      <td>2.455947e-06</td>\n",
       "      <td>5.428416</td>\n",
       "      <td>8.859192e-08</td>\n",
       "      <td>1.155144</td>\n",
       "      <td>2.485799e-01</td>\n",
       "      <td>4.761841</td>\n",
       "      <td>...</td>\n",
       "      <td>5.570164</td>\n",
       "      <td>4.150861e-08</td>\n",
       "      <td>3.470008</td>\n",
       "      <td>5.651810e-04</td>\n",
       "      <td>5.327175</td>\n",
       "      <td>1.507427e-07</td>\n",
       "      <td>-2.575802</td>\n",
       "      <td>1.028509e-02</td>\n",
       "      <td>-3.355312</td>\n",
       "      <td>8.526683e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>114.0</td>\n",
       "      <td>1.810261</td>\n",
       "      <td>7.085192e-02</td>\n",
       "      <td>-3.866473</td>\n",
       "      <td>1.249043e-04</td>\n",
       "      <td>-3.856509</td>\n",
       "      <td>1.299508e-04</td>\n",
       "      <td>5.834720</td>\n",
       "      <td>9.658441e-09</td>\n",
       "      <td>-8.627975</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.815842</td>\n",
       "      <td>1.945979e-17</td>\n",
       "      <td>7.321437</td>\n",
       "      <td>9.793729e-13</td>\n",
       "      <td>0.432093</td>\n",
       "      <td>6.658587e-01</td>\n",
       "      <td>13.434180</td>\n",
       "      <td>2.233484e-35</td>\n",
       "      <td>13.308516</td>\n",
       "      <td>7.768267e-35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>115.0</td>\n",
       "      <td>3.440270</td>\n",
       "      <td>6.294638e-04</td>\n",
       "      <td>6.238650</td>\n",
       "      <td>9.372059e-10</td>\n",
       "      <td>7.879858</td>\n",
       "      <td>2.042314e-14</td>\n",
       "      <td>10.560552</td>\n",
       "      <td>1.107468e-23</td>\n",
       "      <td>1.553167</td>\n",
       "      <td>...</td>\n",
       "      <td>3.261137</td>\n",
       "      <td>1.184921e-03</td>\n",
       "      <td>7.906980</td>\n",
       "      <td>1.683102e-14</td>\n",
       "      <td>6.995820</td>\n",
       "      <td>8.450157e-12</td>\n",
       "      <td>8.327324</td>\n",
       "      <td>7.884092e-16</td>\n",
       "      <td>7.163245</td>\n",
       "      <td>2.816957e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>116.0</td>\n",
       "      <td>6.227919</td>\n",
       "      <td>9.987603e-10</td>\n",
       "      <td>7.551148</td>\n",
       "      <td>2.046147e-13</td>\n",
       "      <td>6.972595</td>\n",
       "      <td>9.825328e-12</td>\n",
       "      <td>7.887942</td>\n",
       "      <td>1.927978e-14</td>\n",
       "      <td>4.039571</td>\n",
       "      <td>...</td>\n",
       "      <td>2.635025</td>\n",
       "      <td>8.672652e-03</td>\n",
       "      <td>4.728078</td>\n",
       "      <td>2.947309e-06</td>\n",
       "      <td>-3.817114</td>\n",
       "      <td>1.518540e-04</td>\n",
       "      <td>2.382272</td>\n",
       "      <td>1.757683e-02</td>\n",
       "      <td>3.310813</td>\n",
       "      <td>9.970701e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>117.0</td>\n",
       "      <td>11.512425</td>\n",
       "      <td>2.201661e-27</td>\n",
       "      <td>8.760171</td>\n",
       "      <td>2.989844e-17</td>\n",
       "      <td>9.758292</td>\n",
       "      <td>1.020031e-20</td>\n",
       "      <td>10.004916</td>\n",
       "      <td>1.298479e-21</td>\n",
       "      <td>-4.895982</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.718983</td>\n",
       "      <td>6.774583e-03</td>\n",
       "      <td>4.612626</td>\n",
       "      <td>5.050583e-06</td>\n",
       "      <td>8.749134</td>\n",
       "      <td>3.254810e-17</td>\n",
       "      <td>7.236588</td>\n",
       "      <td>1.729888e-12</td>\n",
       "      <td>6.700677</td>\n",
       "      <td>5.574122e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>119.0</td>\n",
       "      <td>4.306178</td>\n",
       "      <td>1.997675e-05</td>\n",
       "      <td>2.921090</td>\n",
       "      <td>3.644801e-03</td>\n",
       "      <td>4.822605</td>\n",
       "      <td>1.880571e-06</td>\n",
       "      <td>5.860436</td>\n",
       "      <td>8.357341e-09</td>\n",
       "      <td>-7.612657</td>\n",
       "      <td>...</td>\n",
       "      <td>4.235936</td>\n",
       "      <td>2.707216e-05</td>\n",
       "      <td>9.257343</td>\n",
       "      <td>6.036499e-19</td>\n",
       "      <td>11.127495</td>\n",
       "      <td>7.284697e-26</td>\n",
       "      <td>11.338329</td>\n",
       "      <td>1.080848e-26</td>\n",
       "      <td>7.614421</td>\n",
       "      <td>1.320716e-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    location  dm_nai_mov     p_nai_mov  dm_nai_es      p_nai_es  dm_nai_sa  \\\n",
       "0      123.0    5.572342  4.579726e-08   6.613142  1.185217e-10   6.646138   \n",
       "1      125.0    1.854522  6.676648e-02   0.824226  4.118758e-01   0.026970   \n",
       "2      126.0    0.748011  4.548032e-01   4.046126  6.026210e-05   2.633514   \n",
       "3      127.0    2.001185  4.590922e-02   3.030513  2.567100e-03   3.258509   \n",
       "4      145.0   -0.941103  3.471038e-01   4.766637  2.455947e-06   5.428416   \n",
       "..       ...         ...           ...        ...           ...        ...   \n",
       "15     114.0    1.810261  7.085192e-02  -3.866473  1.249043e-04  -3.856509   \n",
       "16     115.0    3.440270  6.294638e-04   6.238650  9.372059e-10   7.879858   \n",
       "17     116.0    6.227919  9.987603e-10   7.551148  2.046147e-13   6.972595   \n",
       "18     117.0   11.512425  2.201661e-27   8.760171  2.989844e-17   9.758292   \n",
       "19     119.0    4.306178  1.997675e-05   2.921090  3.644801e-03   4.822605   \n",
       "\n",
       "        p_nai_sa  dm_nai_ls      p_nai_ls  dm_mov_es  ...  dm_mov_sa  \\\n",
       "0   9.687066e-11   7.527820  3.351889e-13   4.922440  ...   4.777178   \n",
       "1   9.785399e-01   1.712714  9.002724e-02  -2.625987  ...  -2.579867   \n",
       "2   8.710786e-03   4.564412  6.303586e-06   4.807093  ...   3.306822   \n",
       "3   1.195720e-03   1.658482  9.784354e-02   1.311638  ...   1.616946   \n",
       "4   8.859192e-08   1.155144  2.485799e-01   4.761841  ...   5.570164   \n",
       "..           ...        ...           ...        ...  ...        ...   \n",
       "15  1.299508e-04   5.834720  9.658441e-09  -8.627975  ...  -8.815842   \n",
       "16  2.042314e-14  10.560552  1.107468e-23   1.553167  ...   3.261137   \n",
       "17  9.825328e-12   7.887942  1.927978e-14   4.039571  ...   2.635025   \n",
       "18  1.020031e-20  10.004916  1.298479e-21  -4.895982  ...  -2.718983   \n",
       "19  1.880571e-06   5.860436  8.357341e-09  -7.612657  ...   4.235936   \n",
       "\n",
       "        p_mov_sa  dm_mov_ls      p_mov_ls   dm_es_sa       p_es_sa   dm_es_ls  \\\n",
       "0   2.485800e-06   8.146106  4.660093e-15  -1.420115  1.563399e-01   5.066094   \n",
       "1   1.141633e-02  -0.097533  9.225083e-01  -1.793056  7.614635e-02   1.154491   \n",
       "2   1.011074e-03   5.678262  2.303179e-08  -4.516985  7.823993e-06   1.363633   \n",
       "3   1.065169e-01  -0.046836  9.626622e-01   2.270732  2.358519e-02  -1.263966   \n",
       "4   4.150861e-08   3.470008  5.651810e-04   5.327175  1.507427e-07  -2.575802   \n",
       "..           ...        ...           ...        ...           ...        ...   \n",
       "15  1.945979e-17   7.321437  9.793729e-13   0.432093  6.658587e-01  13.434180   \n",
       "16  1.184921e-03   7.906980  1.683102e-14   6.995820  8.450157e-12   8.327324   \n",
       "17  8.672652e-03   4.728078  2.947309e-06  -3.817114  1.518540e-04   2.382272   \n",
       "18  6.774583e-03   4.612626  5.050583e-06   8.749134  3.254810e-17   7.236588   \n",
       "19  2.707216e-05   9.257343  6.036499e-19  11.127495  7.284697e-26  11.338329   \n",
       "\n",
       "         p_es_ls   dm_sa_ls       p_sa_ls  \n",
       "0   6.170705e-07   5.054385  6.537654e-07  \n",
       "1   2.511949e-01   1.532285  1.287755e-01  \n",
       "2   1.732929e-01   6.390069  3.783688e-10  \n",
       "3   2.068276e-01  -1.643004  1.010071e-01  \n",
       "4   1.028509e-02  -3.355312  8.526683e-04  \n",
       "..           ...        ...           ...  \n",
       "15  2.233484e-35  13.308516  7.768267e-35  \n",
       "16  7.884092e-16   7.163245  2.816957e-12  \n",
       "17  1.757683e-02   3.310813  9.970701e-04  \n",
       "18  1.729888e-12   6.700677  5.574122e-11  \n",
       "19  1.080848e-26   7.614421  1.320716e-13  \n",
       "\n",
       "[108 rows x 21 columns]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dmtestz = dmtest.append(dmtest1)\n",
    "dmtestz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "turned-romantic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>Naive</th>\n",
       "      <th>Moving_Average</th>\n",
       "      <th>Expoential_Smoothing</th>\n",
       "      <th>SARIMA</th>\n",
       "      <th>LSTM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.0</td>\n",
       "      <td>351.532332</td>\n",
       "      <td>239.254357</td>\n",
       "      <td>174.336987</td>\n",
       "      <td>176.475121</td>\n",
       "      <td>103.383646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125.0</td>\n",
       "      <td>258.661651</td>\n",
       "      <td>185.829111</td>\n",
       "      <td>224.253530</td>\n",
       "      <td>257.283002</td>\n",
       "      <td>188.480157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>126.0</td>\n",
       "      <td>84.002446</td>\n",
       "      <td>79.624075</td>\n",
       "      <td>48.370817</td>\n",
       "      <td>64.530111</td>\n",
       "      <td>43.037310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>127.0</td>\n",
       "      <td>70.429155</td>\n",
       "      <td>61.147825</td>\n",
       "      <td>56.685093</td>\n",
       "      <td>55.404851</td>\n",
       "      <td>61.364009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>145.0</td>\n",
       "      <td>91.320776</td>\n",
       "      <td>95.681339</td>\n",
       "      <td>74.905223</td>\n",
       "      <td>72.150681</td>\n",
       "      <td>85.798975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>114.0</td>\n",
       "      <td>313.261116</td>\n",
       "      <td>292.347866</td>\n",
       "      <td>376.453230</td>\n",
       "      <td>375.977332</td>\n",
       "      <td>224.236904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>115.0</td>\n",
       "      <td>398.359378</td>\n",
       "      <td>338.459993</td>\n",
       "      <td>311.973165</td>\n",
       "      <td>286.320937</td>\n",
       "      <td>217.402409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>116.0</td>\n",
       "      <td>221.336806</td>\n",
       "      <td>152.766329</td>\n",
       "      <td>139.490316</td>\n",
       "      <td>144.624937</td>\n",
       "      <td>128.795894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>117.0</td>\n",
       "      <td>165.508798</td>\n",
       "      <td>109.619888</td>\n",
       "      <td>125.616905</td>\n",
       "      <td>117.469585</td>\n",
       "      <td>84.322696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>119.0</td>\n",
       "      <td>680.060399</td>\n",
       "      <td>411.837616</td>\n",
       "      <td>507.154884</td>\n",
       "      <td>362.243897</td>\n",
       "      <td>237.480780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    location       Naive  Moving_Average  Expoential_Smoothing      SARIMA  \\\n",
       "0      123.0  351.532332      239.254357            174.336987  176.475121   \n",
       "1      125.0  258.661651      185.829111            224.253530  257.283002   \n",
       "2      126.0   84.002446       79.624075             48.370817   64.530111   \n",
       "3      127.0   70.429155       61.147825             56.685093   55.404851   \n",
       "4      145.0   91.320776       95.681339             74.905223   72.150681   \n",
       "..       ...         ...             ...                   ...         ...   \n",
       "15     114.0  313.261116      292.347866            376.453230  375.977332   \n",
       "16     115.0  398.359378      338.459993            311.973165  286.320937   \n",
       "17     116.0  221.336806      152.766329            139.490316  144.624937   \n",
       "18     117.0  165.508798      109.619888            125.616905  117.469585   \n",
       "19     119.0  680.060399      411.837616            507.154884  362.243897   \n",
       "\n",
       "          LSTM  \n",
       "0   103.383646  \n",
       "1   188.480157  \n",
       "2    43.037310  \n",
       "3    61.364009  \n",
       "4    85.798975  \n",
       "..         ...  \n",
       "15  224.236904  \n",
       "16  217.402409  \n",
       "17  128.795894  \n",
       "18   84.322696  \n",
       "19  237.480780  \n",
       "\n",
       "[108 rows x 6 columns]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_accuracyz = pred_accuracy.append(pred_accuracy1)\n",
    "pred_accuracyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "serial-voluntary",
   "metadata": {},
   "outputs": [],
   "source": [
    "dmtestz.to_csv('dmtest.csv')\n",
    "pred_accuracyz.to_csv('pred_accuracy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-international",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urbsim",
   "language": "python",
   "name": "urbsim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
